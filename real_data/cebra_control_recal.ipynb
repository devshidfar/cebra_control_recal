{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "sys.path.append(\"/Users/devenshidfar/Desktop/Masters/NRSC_510B/cebra_control_recal\")\n",
    "#sys.path.remove(\"/Users/devenshidfar/Desktop/Masters/NRSC_510B/cebra_control_recal/shared_scripts\")\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import cebra\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import seaborn as sns; sns.set()\n",
    "import timeit\n",
    "from scipy.interpolate import splprep, splev\n",
    "# import manifold_fit_and_decode_fns as mff\n",
    "# import fit_helper_fns as fhf\n",
    "import cebra_utils\n",
    "from cebra_utils import *\n",
    "importlib.reload(cebra_utils)\n",
    "from scipy.interpolate import interp1d\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65,)\n",
      "Experiment data type: object\n",
      "Experiment data shape: (65,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "file_path = '/Users/devenshidfar/Desktop/Masters/NRSC_510B/cebra_control_recal/mat_code_and_data/data/NN_opticflow_dataset.mat'\n",
    "data = scipy.io.loadmat(file_path, squeeze_me=True, struct_as_record=False)\n",
    "expt = data['expt']\n",
    "\n",
    "print(expt.shape)\n",
    "\n",
    "print(f\"Experiment data type: {expt.dtype}\")\n",
    "print(f\"Experiment data shape: {expt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing bin_size: 1 second(s)\n",
      "Skipping session 1\n",
      "Skipping session 2\n",
      "Skipping session 3\n",
      "Skipping session 4\n",
      "Skipping session 5\n",
      "Skipping session 6\n",
      "Skipping session 7\n",
      "Skipping session 8\n",
      "Skipping session 9\n",
      "Skipping session 10\n",
      "Skipping session 11\n",
      "Skipping session 12\n",
      "Skipping session 13\n",
      "Skipping session 14\n",
      "Skipping session 15\n",
      "Skipping session 16\n",
      "Skipping session 17\n",
      "Skipping session 18\n",
      "Skipping session 19\n",
      "Skipping session 20\n",
      "Skipping session 21\n",
      "Skipping session 22\n",
      "Skipping session 23\n",
      "Skipping session 24\n",
      "Skipping session 25\n",
      "Skipping session 26\n",
      "Skipping session 27\n",
      "Skipping session 28\n",
      "Skipping session 29\n",
      "Skipping session 30\n",
      "Skipping session 31\n",
      "Skipping session 32\n",
      "Skipping session 33\n",
      "Skipping session 34\n",
      "Skipping session 35\n",
      "Skipping session 36\n",
      "Skipping session 37\n",
      "Skipping session 38\n",
      "Skipping session 39\n",
      "\n",
      "Processing session 40/65\n",
      "Rat: 913, Day: 7, Epoch: m1\n",
      "Initial hippocampus angle binned: [1706.28325428 1729.75127575 1756.16145355 1783.976162   1800.89865898\n",
      " 1819.40486542 1855.01657053 1872.88707102 1886.97503837 1916.05679869\n",
      " 1942.43626894 1961.36441374 1977.69699991           nan           nan\n",
      "           nan           nan 1994.16087722 2027.84391428 2045.35865782]\n",
      "NaN counts in hipp_angle_binned: 250\n",
      "Empty bins: [  13   14   15   16   23   24   49   64   71   72   87  105  126  160\n",
      "  187  244  285  288  302  327  328  359  368  403  415  429  438  469\n",
      "  470  471  472  473  474  475  476  477  478  479  492  505  506  507\n",
      "  508  520  552  560  564  568  580  581  582  583  584  590  591  592\n",
      "  603  604  609  622  623  624  640  658  673  674  718  719  720  721\n",
      "  722  729  747  764  779  780  813  838  844  851  864  868  874  877\n",
      "  878  883  889  897  939  942  943  944  945 1014 1015 1016 1032 1036\n",
      " 1039 1045 1048 1053 1064 1071 1072 1073 1074 1081 1082 1112 1120 1124\n",
      " 1125 1138 1139 1140 1141 1142 1143 1153 1170 1209 1210 1211 1212 1213\n",
      " 1214 1227 1233 1236 1240 1258 1259 1263 1285 1354 1381 1382 1383 1384\n",
      " 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1403 1425 1438 1454\n",
      " 1456 1466 1470 1496 1509 1515 1526 1535 1536 1537 1545 1554 1564 1570\n",
      " 1594 1601 1605 1617 1621 1625 1628 1633 1646 1652 1681 1682 1683 1684\n",
      " 1685 1686 1687 1688 1689 1690 1691 1698 1707 1718 1725 1726 1727 1728\n",
      " 1729 1730 1731 1737 1746 1747 1748 1749 1750 1751 1757 1758 1759 1760\n",
      " 1761 1762 1765 1766 1767 1774 1777 1778 1779 1780 1781 1800 1806 1816\n",
      " 1820 1826 1836 1855 1871 1878 1902 1944 1950 1963 1964 1981 1991 2002\n",
      " 2009 2034 2043 2044 2051 2059 2065 2069 2078 2080 2081 2088]\n",
      "Skipping cluster TT14/cl-maze1.8 due to low isolation quality (4)\n",
      "Skipping cluster TT14/cl-maze1.9 due to low isolation quality (4)\n",
      "Skipping cluster TT15/cl-maze1.1 due to low isolation quality (4)\n",
      "Skipping cluster TT16/cl-maze1.10 due to low isolation quality (4)\n",
      "Skipping cluster TT16/cl-maze1.5 due to low isolation quality (4)\n",
      "Skipping cluster TT4/cl-maze1.4 due to low isolation quality (4)\n",
      "Skipping cluster TT7/cl-maze1.11 due to low isolation quality (4)\n",
      "Skipping cluster TT7/cl-maze1.13 due to low isolation quality (4)\n",
      "Skipping cluster TT7/cl-maze1.14 due to low isolation quality (4)\n",
      "Skipping cluster TT7/cl-maze1.17 due to low isolation quality (4)\n",
      "Output embeddings_2d shape before nt_TDA: (2091, 2)\n",
      "Output embeddings_3d shape before nt_TDA: (2091, 3)\n",
      "removing outliers...\n",
      "Pairwise distance matrix computed.\n",
      "Distance matrix shape: (2091, 2091)\n",
      "Sample distances (first 5 rows, first 5 columns):\n",
      "[[10.          0.62266658  0.99350955  1.21885481  1.19101955]\n",
      " [ 0.62266658 10.          0.40372619  0.66460254  0.63160877]\n",
      " [ 0.99350955  0.40372619 10.          0.27013713  0.23554109]\n",
      " [ 1.21885481  0.66460254  0.27013713 10.          0.03487456]\n",
      " [ 1.19101955  0.63160877  0.23554109  0.03487456 10.        ]]\n",
      "\n",
      "Neighborhood radius calculated using the 1th percentile.\n",
      "Neighborhood radius statistics:\n",
      "  Min: 0.0\n",
      "  Max: 0.11230619508110844\n",
      "  Mean: 0.0319\n",
      "  Median: 0.0311\n",
      "\n",
      "Neighbor counts computed for each point.\n",
      "Neighbor counts statistics:\n",
      "  Min: 21\n",
      "  Max: 336\n",
      "  Mean: 68.85\n",
      "  Median: 21.0\n",
      "  Example neighbor counts (first 10 points): [ 21  21  21  21  21  21  21  21  21  21  21  21  21 315 315 315 315  21\n",
      "  21 319  21  21  21 315 315  21  21  21  21  21  21  21  21  21  21  21\n",
      " 334 323  21  21  21  21  21  21  21  21  21  21  21 315]\n",
      "\n",
      "Threshold for neighbor counts set at the 20th percentile: 21.0\n",
      "Outliers based on neighbor counts (count=0): []\n",
      "\n",
      "Minimum distance to any other point calculated for each point.\n",
      "Minimum distance statistics:\n",
      "  Min: 11.301018938782672\n",
      "  Max: 11.315650889148653\n",
      "  Mean: 11.3032\n",
      "  Median: 11.3034\n",
      "\n",
      "Distance threshold set at the 90th percentile: 11.30443373601961\n",
      "Outliers based on distance threshold (count=174): [  20   35   36   38   50   51   80   82   96   99  110  139  141  154\n",
      "  169  170  199  217  245  271  283  361  367  377  397  424  439  482\n",
      "  543  556  596  613  630  675  689  706  727  741  742  757  784  790\n",
      "  803  804  816  834  850  866  899 1041 1077 1096 1108 1114 1147 1161\n",
      " 1191 1216 1217 1231 1249 1269 1287 1303 1304 1307 1322 1323 1335 1349\n",
      " 1351 1367 1427 1440 1441 1458 1476 1506 1539 1555 1574 1586 1589 1601\n",
      " 1604 1605 1617 1620 1621 1625 1633 1637 1646 1652 1655 1657 1670 1672\n",
      " 1674 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1695 1696 1697\n",
      " 1698 1699 1718 1725 1726 1727 1728 1729 1730 1731 1737 1746 1747 1748\n",
      " 1749 1750 1751 1757 1758 1759 1760 1761 1762 1765 1766 1767 1768 1774\n",
      " 1777 1778 1779 1780 1781 1782 1788 1789 1790 1792 1802 1806 1826 1837\n",
      " 1838 1839 1840 1885 1886 1887 1937 1947 1968 1985 2003 2017 2018 2019\n",
      " 2036 2052 2053 2072 2075 2089]\n",
      "\n",
      "Total outliers detected after combining criteria (count=174): [  20   35   36   38   50   51   80   82   96   99  110  139  141  154\n",
      "  169  170  199  217  245  271  283  361  367  377  397  424  439  482\n",
      "  543  556  596  613  630  675  689  706  727  741  742  757  784  790\n",
      "  803  804  816  834  850  866  899 1041 1077 1096 1108 1114 1147 1161\n",
      " 1191 1216 1217 1231 1249 1269 1287 1303 1304 1307 1322 1323 1335 1349\n",
      " 1351 1367 1427 1440 1441 1458 1476 1506 1539 1555 1574 1586 1589 1601\n",
      " 1604 1605 1617 1620 1621 1625 1633 1637 1646 1652 1655 1657 1670 1672\n",
      " 1674 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1695 1696 1697\n",
      " 1698 1699 1718 1725 1726 1727 1728 1729 1730 1731 1737 1746 1747 1748\n",
      " 1749 1750 1751 1757 1758 1759 1760 1761 1762 1765 1766 1767 1768 1774\n",
      " 1777 1778 1779 1780 1781 1782 1788 1789 1790 1792 1802 1806 1826 1837\n",
      " 1838 1839 1840 1885 1886 1887 1937 1947 1968 1985 2003 2017 2018 2019\n",
      " 2036 2052 2053 2072 2075 2089]\n",
      "\n",
      "Total inliers detected (count=1917): [   0    1    2 ... 2087 2088 2090]\n",
      "\n",
      "Pairwise distance matrix computed.\n",
      "Distance matrix shape: (2091, 2091)\n",
      "Sample distances (first 5 rows, first 5 columns):\n",
      "[[10.          0.50938739  0.93774432  1.35252598  1.42566415]\n",
      " [ 0.50938739 10.          0.46650787  0.948407    1.02139874]\n",
      " [ 0.93774432  0.46650787 10.          0.5117231   0.61587611]\n",
      " [ 1.35252598  0.948407    0.5117231  10.          0.26305572]\n",
      " [ 1.42566415  1.02139874  0.61587611  0.26305572 10.        ]]\n",
      "\n",
      "Neighborhood radius calculated using the 1th percentile.\n",
      "Neighborhood radius statistics:\n",
      "  Min: 0.0\n",
      "  Max: 0.5157363467438476\n",
      "  Mean: 0.0807\n",
      "  Median: 0.0771\n",
      "\n",
      "Neighbor counts computed for each point.\n",
      "Neighbor counts statistics:\n",
      "  Min: 21\n",
      "  Max: 316\n",
      "  Mean: 65.61\n",
      "  Median: 21.0\n",
      "  Example neighbor counts (first 10 points): [ 21  21  21  21  21  21  21  21  21  21  21  21  21 315 315 315 315  21\n",
      "  21  21  21  21  21 315 315  21  21  21  21  21  21  21  21  21  21  21\n",
      "  21  21  21  21  21  21  21  21  21  21  21  21  21 315]\n",
      "\n",
      "Threshold for neighbor counts set at the 20th percentile: 21.0\n",
      "Outliers based on neighbor counts (count=0): []\n",
      "\n",
      "Minimum distance to any other point calculated for each point.\n",
      "Minimum distance statistics:\n",
      "  Min: 11.244534334853245\n",
      "  Max: 13.286824868050346\n",
      "  Mean: 11.2816\n",
      "  Median: 11.2725\n",
      "\n",
      "Distance threshold set at the 90th percentile: 11.313708689719636\n",
      "Outliers based on distance threshold (count=78): [  10  183  210  239  296  313  326  413  524  572  604  764  780  828\n",
      "  844  874  895  941  952  959  970 1036 1082 1104 1127 1178 1204 1210\n",
      " 1212 1221 1223 1229 1240 1244 1246 1257 1258 1277 1283 1304 1336 1354\n",
      " 1379 1384 1386 1388 1400 1431 1458 1466 1489 1496 1532 1555 1564 1594\n",
      " 1620 1628 1722 1763 1776 1788 1789 1820 1836 1894 1903 1904 1952 1968\n",
      " 1985 2017 2037 2053 2071 2085 2087 2089]\n",
      "\n",
      "Total outliers detected after combining criteria (count=78): [  10  183  210  239  296  313  326  413  524  572  604  764  780  828\n",
      "  844  874  895  941  952  959  970 1036 1082 1104 1127 1178 1204 1210\n",
      " 1212 1221 1223 1229 1240 1244 1246 1257 1258 1277 1283 1304 1336 1354\n",
      " 1379 1384 1386 1388 1400 1431 1458 1466 1489 1496 1532 1555 1564 1594\n",
      " 1620 1628 1722 1763 1776 1788 1789 1820 1836 1894 1903 1904 1952 1968\n",
      " 1985 2017 2037 2053 2071 2085 2087 2089]\n",
      "\n",
      "Total inliers detected (count=2013): [   0    1    2 ... 2086 2088 2090]\n",
      "\n",
      "Output embeddings_2d shape after nt_TDA: (1917, 2)\n",
      "Output embeddings_3d shape after nt_TDA: (2013, 3)\n",
      " hippocampal angle: [196.3634675  225.9021174  254.68897981 270.5978619  307.90074489\n",
      " 350.00193278   3.17653334  27.48811625  58.8859578   78.96688357\n",
      "  89.6512154  114.93949905 151.70950976          nan 178.02149709\n",
      " 213.84591178 251.75498807 267.04233134 278.94775554 300.57965714\n",
      "          nan          nan 311.07956886 344.61786756  31.39928908\n",
      "  57.87489963  68.94709496 104.19605196 143.82223763 157.78671336\n",
      " 183.02186877 229.47101179 259.00439109 270.73175797 306.04607857\n",
      " 349.47008625          nan  18.60530647  37.55942787  49.81560277\n",
      "  81.62163049 110.4213694  122.26306557 144.23846079 180.6479673\n",
      " 217.79844665 228.79136452 253.45349157 295.72708479 319.96041283])\n",
      "dense points: [[-0.2793754  -0.29906335  0.91242009]\n",
      " [-0.31951576 -0.7152825   0.6215148 ]\n",
      " [-0.16812961 -0.95351368  0.25008819]\n",
      " ...\n",
      " [-0.26910019  0.26492298  0.92595947]\n",
      " [-0.97909302  0.04739185 -0.19781519]\n",
      " [-0.97909302  0.04739185 -0.19781519]]\n",
      "made it here\n",
      "method: kmedoids\n",
      "Doing K-Medoids initial clustering\n",
      "cluster centers: [[ 0.4467926   0.34408602 -0.82582152]\n",
      " [-0.31886718  0.27928498  0.90571713]\n",
      " [-0.10119871 -0.9933663  -0.05460989]\n",
      " [ 0.28442699  0.93743813  0.20077589]\n",
      " [ 0.3226217  -0.64335167 -0.69427216]\n",
      " [-0.52825135 -0.63376409  0.56506079]\n",
      " [ 0.11218528  0.863442    0.49181533]\n",
      " [ 0.3353411   0.91556084 -0.22202383]\n",
      " [ 0.45191288 -0.15308961 -0.87882781]\n",
      " [-0.3809886  -0.03219749  0.92401892]\n",
      " [ 0.12982044 -0.91351902 -0.38552508]\n",
      " [-0.13910855  0.59822536  0.78916109]\n",
      " [-0.32796714 -0.9196142   0.21621123]\n",
      " [ 0.50797719  0.63251388 -0.58470958]\n",
      " [-0.97909302  0.04739185 -0.19781519]]\n",
      "Saved initial knots plot to /Users/devenshidfar/Desktop/Masters/NRSC_510B/cebra_control_recal/results/temperature_1.5/initial_knots_session_39.png\n",
      "curr_fit_params: {'init_knots': array([[ 0.45191288, -0.15308961, -0.87882781],\n",
      "       [ 0.4467926 ,  0.34408602, -0.82582152],\n",
      "       [ 0.50797719,  0.63251388, -0.58470958],\n",
      "       [ 0.3353411 ,  0.91556084, -0.22202383],\n",
      "       [ 0.28442699,  0.93743813,  0.20077589],\n",
      "       [ 0.11218528,  0.863442  ,  0.49181533],\n",
      "       [-0.13910855,  0.59822536,  0.78916109],\n",
      "       [-0.31886718,  0.27928498,  0.90571713],\n",
      "       [-0.3809886 , -0.03219749,  0.92401892],\n",
      "       [-0.52825135, -0.63376409,  0.56506079],\n",
      "       [-0.32796714, -0.9196142 ,  0.21621123],\n",
      "       [-0.10119871, -0.9933663 , -0.05460989],\n",
      "       [ 0.12982044, -0.91351902, -0.38552508],\n",
      "       [ 0.3226217 , -0.64335167, -0.69427216],\n",
      "       [-0.97909302,  0.04739185, -0.19781519]]), 'dalpha': 0.005, 'knot_order': 'nearest', 'penalty_type': 'none', 'nKnots': 15, 'curvature_coeff': 1, 'len_coeff': 2, 'density_coeff': 2, 'delta': 0.1}\n",
      "fit_params in fit_data: {'init_knots': array([[ 0.45191288, -0.15308961, -0.87882781],\n",
      "       [ 0.4467926 ,  0.34408602, -0.82582152],\n",
      "       [ 0.50797719,  0.63251388, -0.58470958],\n",
      "       [ 0.3353411 ,  0.91556084, -0.22202383],\n",
      "       [ 0.28442699,  0.93743813,  0.20077589],\n",
      "       [ 0.11218528,  0.863442  ,  0.49181533],\n",
      "       [-0.13910855,  0.59822536,  0.78916109],\n",
      "       [-0.31886718,  0.27928498,  0.90571713],\n",
      "       [-0.3809886 , -0.03219749,  0.92401892],\n",
      "       [-0.52825135, -0.63376409,  0.56506079],\n",
      "       [-0.32796714, -0.9196142 ,  0.21621123],\n",
      "       [-0.10119871, -0.9933663 , -0.05460989],\n",
      "       [ 0.12982044, -0.91351902, -0.38552508],\n",
      "       [ 0.3226217 , -0.64335167, -0.69427216],\n",
      "       [-0.97909302,  0.04739185, -0.19781519]]), 'dalpha': 0.005, 'knot_order': 'nearest', 'penalty_type': 'none', 'nKnots': 15, 'curvature_coeff': 1, 'len_coeff': 2, 'density_coeff': 2, 'delta': 0.1}\n",
      "init_knots shape: (15, 3)\n",
      "flat_init_knots size: 45\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n",
      "delta is: 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 223\u001b[0m\n\u001b[1;32m    212\u001b[0m fit_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdalpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.005\u001b[39m,\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknot_order\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m    221\u001b[0m }\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# principal_curve_2d, principal_curve_2d_pre, curve_params_2d, decoded_angles_2d, mse_2d = fit_spud_to_cebra(embeddings_2d,ref_angles=hipp_angle_binned,hippocampal_angle_origin=0, session_idx=session_idx,session=session,results_save_path=results_save_path,dimension_3d=0)\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m principal_curve_3d, principal_curve_3d_pre, curve_params_3d, decoded_angles_3d, mse_3d \u001b[38;5;241m=\u001b[39m \u001b[43mfit_spud_to_cebra\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43mref_angles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhipp_angle_binned_3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhippocampal_angle_origin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresults_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdimension_3d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# # Interpolate the principal curves (for calculating dists between the embedding points and princ curve)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# interpolated_curve_2d = interpolate_principal_curve(principal_curve_2d, points_per_unit_distance=10)\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# interpolated_curve_3d = interpolate_principal_curve(principal_curve_3d, points_per_unit_distance=10)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m \n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Create rotating 3D plots\u001b[39;00m\n\u001b[1;32m    297\u001b[0m anim_save_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manim_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bin_size-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbin_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/session_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msession_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/vel_thresh_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvel_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/curv_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pen_type_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpenalty_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Masters/NRSC_510B/cebra_control_recal/cebra_utils.py:308\u001b[0m, in \u001b[0;36mfit_spud_to_cebra\u001b[0;34m(embeddings, ref_angles, hippocampal_angle_origin, session_idx, session, results_save_path, fit_params, dimension_3d)\u001b[0m\n\u001b[1;32m    306\u001b[0m curr_fit_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_knots\u001b[39m\u001b[38;5;124m'\u001b[39m: init_knots, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params}\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurr_fit_params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_fit_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 308\u001b[0m \u001b[43mfitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_fit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Get the final knots\u001b[39;00m\n\u001b[1;32m    311\u001b[0m final_knots \u001b[38;5;241m=\u001b[39m fitter\u001b[38;5;241m.\u001b[39msaved_knots[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknots\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Masters/NRSC_510B/cebra_control_recal/spud_code/shared_scripts/manifold_fit_and_decode_fns_custom.py:206\u001b[0m, in \u001b[0;36mPiecewiseLinearFit.fit_data\u001b[0;34m(self, fit_params)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflat_init_knots size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, flat_init_knots\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m    204\u001b[0m bound_cost_fn \u001b[38;5;241m=\u001b[39m partial(cost_fn, fit_params\u001b[38;5;241m=\u001b[39mfit_params)\n\u001b[0;32m--> 206\u001b[0m fit_result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbound_cost_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_init_knots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNelder-Mead\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m knots \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(fit_result\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mcopy(), (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnKnots, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnDims))\n\u001b[1;32m    214\u001b[0m save_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknots\u001b[39m\u001b[38;5;124m'\u001b[39m: knots, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merr\u001b[39m\u001b[38;5;124m'\u001b[39m: fit_result\u001b[38;5;241m.\u001b[39mfun, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_knots\u001b[39m\u001b[38;5;124m'\u001b[39m: init_knots}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NRSC510/lib/python3.9/site-packages/scipy/optimize/_minimize.py:701\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    698\u001b[0m callback \u001b[38;5;241m=\u001b[39m _wrap_callback(callback, meth)\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnelder-mead\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 701\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_neldermead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m                               \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpowell\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    704\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_powell(fun, x0, args, callback, bounds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NRSC510/lib/python3.9/site-packages/scipy/optimize/_optimize.py:843\u001b[0m, in \u001b[0;36m_minimize_neldermead\u001b[0;34m(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, adaptive, bounds, **unknown_options)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bounds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    842\u001b[0m     xr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(xr, lower_bound, upper_bound)\n\u001b[0;32m--> 843\u001b[0m fxr \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m doshrink \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fxr \u001b[38;5;241m<\u001b[39m fsim[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NRSC510/lib/python3.9/site-packages/scipy/optimize/_optimize.py:526\u001b[0m, in \u001b[0;36m_wrap_scalar_function_maxfun_validation.<locals>.function_wrapper\u001b[0;34m(x, *wrapper_args)\u001b[0m\n\u001b[1;32m    524\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# A copy of x is sent to the user function (gh13740)\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwrapper_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# Ideally, we'd like to a have a true scalar returned from f(x). For\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# backwards-compatibility, also allow np.array([1.3]),\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# np.array([[1.3]]) etc.\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/Desktop/Masters/NRSC_510B/cebra_control_recal/spud_code/shared_scripts/manifold_fit_and_decode_fns_custom.py:148\u001b[0m, in \u001b[0;36mPiecewiseLinearFit.fit_data.<locals>.cost_fn\u001b[0;34m(flat_knots, fit_params)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Compute data density using Kernel Density Estimation\u001b[39;00m\n\u001b[1;32m    147\u001b[0m kde \u001b[38;5;241m=\u001b[39m KernelDensity(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgaussian\u001b[39m\u001b[38;5;124m'\u001b[39m, bandwidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_to_fit)\n\u001b[0;32m--> 148\u001b[0m log_density_data \u001b[38;5;241m=\u001b[39m \u001b[43mkde\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_to_fit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m density_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(log_density_data)\n\u001b[1;32m    150\u001b[0m weights \u001b[38;5;241m=\u001b[39m density_data \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(density_data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NRSC510/lib/python3.9/site-packages/sklearn/neighbors/_kde.py:272\u001b[0m, in \u001b[0;36mKernelDensity.score_samples\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    270\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39msum_weight\n\u001b[1;32m    271\u001b[0m atol_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matol \u001b[38;5;241m*\u001b[39m N\n\u001b[0;32m--> 272\u001b[0m log_density \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_density\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbandwidth_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol_N\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbreadth_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbreadth_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m log_density \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(N)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_density\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import pickle  # For saving filtered expt\n",
    "import cebra\n",
    "\n",
    "# config Parameters\n",
    "num_trials = 40\n",
    "control_point = 39\n",
    "control_count = 0\n",
    "\n",
    "model_save_path = '/Users/devenshidfar/Desktop/Masters/NRSC_510B/cebra_control_recal/models'\n",
    "\n",
    "\n",
    "# Set to 1 to save the models and animations\n",
    "save_models = 1\n",
    "save_anim = 1\n",
    "load_npy = 0\n",
    "rm_outliers = True\n",
    "\n",
    "\n",
    "# Define bin sizes (in seconds)\n",
    "bin_sizes = [1]  # You can add more bin sizes if needed\n",
    "\n",
    "# Initialize lists to store embeddings and metadata\n",
    "embeddings_list = []\n",
    "metadata_list = []\n",
    "\n",
    "# Define velocity threshold\n",
    "vel_threshold = 5 # degrees per second\n",
    "\n",
    "# Main Processing Loop\n",
    "for bin_size in bin_sizes:\n",
    "    print(f\"\\nProcessing bin_size: {bin_size} second(s)\")\n",
    "\n",
    "    for session_idx, session in enumerate(expt):\n",
    "        control_count += 1\n",
    "\n",
    "        # Control Session Skipping\n",
    "        if control_count <= control_point:\n",
    "            print(f\"Skipping session {session_idx + 1}\")\n",
    "            continue \n",
    "        elif control_count > control_point + num_trials:\n",
    "            print(\"Reached the desired number of trials. Exiting session loop.\")\n",
    "            break\n",
    "\n",
    "        print(f\"\\nProcessing session {session_idx + 1}/{len(expt)}\")\n",
    "        print(f\"Rat: {session.rat}, Day: {session.day}, Epoch: {session.epoch}\")\n",
    "\n",
    "        ros_data = session.rosdata\n",
    "\n",
    "        start_time = ros_data.startTs\n",
    "        end_time = ros_data.stopTs\n",
    "\n",
    "        # Convert encTimes to seconds\n",
    "        enc_times = np.array(ros_data.encTimes-start_time) / 1e6  # Convert to seconds\n",
    "        vel = np.array(ros_data.vel)\n",
    "\n",
    "        # Ensure enc_times and vel are valid\n",
    "        valid_idx = np.isfinite(enc_times) & np.isfinite(vel)\n",
    "        enc_times = enc_times[valid_idx]\n",
    "        vel = vel[valid_idx]\n",
    "\n",
    "        # Get indices where vel > vel_threshold\n",
    "        high_vel_idx = vel > vel_threshold\n",
    "\n",
    "        if np.sum(high_vel_idx) == 0:\n",
    "            print(\"No data points where vel > vel_threshold. Skipping session.\")\n",
    "            continue\n",
    "\n",
    "        # Filter enc_times and behavioral variables\n",
    "        enc_times_high_vel = enc_times[high_vel_idx]\n",
    "        high_vel = vel[high_vel_idx]\n",
    "        est_gain = np.array(ros_data.estGain)[valid_idx][high_vel_idx]\n",
    "        hipp_angle = np.array(ros_data.hippAngle)[valid_idx][high_vel_idx]\n",
    "        true_angle = np.array(ros_data.encAngle)[valid_idx][high_vel_idx]\n",
    "\n",
    "        # Define bins over enc_times_high_vel\n",
    "        bins = np.arange(enc_times_high_vel[0], enc_times_high_vel[-1] + bin_size, bin_size)\n",
    "\n",
    "        if len(bins) < 2:\n",
    "            print(\"Not enough data after filtering for high velocity. Skipping session.\")\n",
    "            continue\n",
    "\n",
    "        # Bin behavioral variables\n",
    "        est_gain_binned, _, _ = stats.binned_statistic(enc_times_high_vel, est_gain, statistic='mean', bins=bins)\n",
    "        hipp_angle_binned, _, _ = stats.binned_statistic(enc_times_high_vel, hipp_angle, statistic='mean', bins=bins)\n",
    "        true_angle_binned, _, _ = stats.binned_statistic(enc_times_high_vel, true_angle, statistic='mean', bins=bins)\n",
    "        high_vel_binned, _, _ = stats.binned_statistic(enc_times_high_vel, high_vel, statistic='mean', bins=bins)\n",
    "\n",
    "        # Check for NaNs in binned data\n",
    "        print(f\"Initial hippocampus angle binned: {hipp_angle_binned[:20]}\")\n",
    "        print(\"NaN counts in hipp_angle_binned:\", np.isnan(hipp_angle_binned).sum())\n",
    "        print(\"Empty bins:\", np.where(np.isnan(hipp_angle_binned))[0])\n",
    "\n",
    "\n",
    "        # #remove NaN bins\n",
    "        # valid_bins = ~np.isnan(hipp_angle_binned)\n",
    "        # hipp_angle_binned = hipp_angle_binned[valid_bins]\n",
    "        # est_gain_binned = est_gain_binned[valid_bins]\n",
    "        # true_angle_binned = true_angle_binned[valid_bins]\n",
    "        # high_vel_binned = high_vel_binned[valid_bins]\n",
    "\n",
    "        # Filter Spike Times Based on High Velocity\n",
    "        \n",
    "        all_spikes = []\n",
    "        skipped_clusters = 0\n",
    "\n",
    "        for cluster in session.clust:\n",
    "            if cluster.isolationQuality > 3:\n",
    "                print(f\"Skipping cluster {cluster.name} due to low isolation quality ({cluster.isolationQuality})\") #done in Madhav et al., 2024\n",
    "                skipped_clusters += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "            spike_times_sec = (cluster.ts - start_time) / 1e6  # Convert to seconds\n",
    "\n",
    "            vel_at_spikes = np.interp(spike_times_sec, enc_times, vel)\n",
    "    \n",
    "\n",
    "            # Include spikes where vel_at_spikes > vel_threshold\n",
    "            include_spikes = vel_at_spikes > vel_threshold\n",
    "            spike_times_sec_high_vel = spike_times_sec[include_spikes]\n",
    "\n",
    "            if len(spike_times_sec_high_vel) == 0:\n",
    "                print(f\"No spikes for cluster {cluster.name} at high velocities. Skipping cluster.\")\n",
    "                continue\n",
    "\n",
    "            # Bin spikes\n",
    "            binned_spikes, _, _ = stats.binned_statistic(spike_times_sec_high_vel, np.ones_like(spike_times_sec_high_vel), statistic='sum', bins=bins)\n",
    "\n",
    "            all_spikes.append(binned_spikes)\n",
    "\n",
    "        if not all_spikes:\n",
    "            print(\"No valid spike data after filtering. Skipping session.\")\n",
    "            continue\n",
    "\n",
    "        # Assemble neural data\n",
    "        neural_data = np.array(all_spikes).T  # Shape: (num_bins, num_clusters)\n",
    "\n",
    "        # Consistency Check Between Bins\n",
    "        num_bins_neural = neural_data.shape[0]\n",
    "        num_bins_behavior = len(est_gain_binned)\n",
    "\n",
    "        if num_bins_neural != num_bins_behavior:\n",
    "            print(f\"Warning: Number of neural data bins ({num_bins_neural}) does not match behavioral data bins ({num_bins_behavior}). Adjusting to minimum.\")\n",
    "            min_bins = min(num_bins_neural, num_bins_behavior)\n",
    "            neural_data = neural_data[:min_bins, :]\n",
    "            est_gain_binned = est_gain_binned[:min_bins]\n",
    "            hipp_angle_binned = hipp_angle_binned[:min_bins] * ((2*np.pi)/360)\n",
    "            true_angle_binned = true_angle_binned[:min_bins] * ((2*np.pi)/360)\n",
    "            bins = bins[:min_bins+1]\n",
    "\n",
    "   \n",
    "        embeddings_2d_list = []\n",
    "        embeddings_3d_list = []\n",
    "\n",
    "        embedding_filename_2d = f\"bin_size-{bin_size}_embeddings_2d_rat{session.rat}_day{session.day}_epoch{session.epoch}.npy\"\n",
    "        embedding_filename_3d = f\"bin_size-{bin_size}_embeddings_3d_rat{session.rat}_day{session.day}_epoch{session.epoch}.npy\"\n",
    "\n",
    "        # if load_npy == 1: #NOT WORKING\n",
    "        #     # Load embeddings data from .npy files as numpy arrays\n",
    "        #     embeddings_2d_data = np.load(os.path.join(model_save_path, embedding_filename_2d))\n",
    "        #     embeddings_3d_data = np.load(os.path.join(model_save_path, embedding_filename_3d))\n",
    "        #     print(f\"Loaded embeddings data from: {embedding_filename_2d}, {embedding_filename_3d}\")\n",
    "\n",
    "        # else:\n",
    "        #     # Apply CEBRA in 2D and 3D\n",
    "        temperature = [1.5,2]\n",
    "        for temp in temperature:\n",
    "            results_save_path = f'/Users/devenshidfar/Desktop/Masters/NRSC_510B/cebra_control_recal/results/temperature_{temp}'\n",
    "            anim_save_path = f'{results_save_path}/3d_animations'\n",
    "            time_dist_save_path = f'{results_save_path}/time_dist_plot'\n",
    "            # Create necessary directories\n",
    "            os.makedirs(model_save_path, exist_ok=True)\n",
    "            os.makedirs(anim_save_path, exist_ok=True)\n",
    "            os.makedirs(results_save_path, exist_ok=True)\n",
    "            embeddings_2d = apply_cebra(neural_data, 2,temperature=temp)\n",
    "            embeddings_3d = apply_cebra(neural_data, 3,temperature=temp)\n",
    "\n",
    "            print(f\"Output embeddings_2d shape before nt_TDA: {embeddings_2d.shape}\")\n",
    "            print(f\"Output embeddings_3d shape before nt_TDA: {embeddings_3d.shape}\")\n",
    "            if(rm_outliers == True):\n",
    "                print(\"removing outliers...\")\n",
    "                inlier_indices_2d = nt_TDA(embeddings_2d)\n",
    "                inlier_indices_3d = nt_TDA(embeddings_3d)\n",
    "                embeddings_2d = embeddings_2d[inlier_indices_2d]\n",
    "                embeddings_3d = embeddings_3d[inlier_indices_3d]\n",
    "                hipp_angle_binned_3d = hipp_angle_binned[inlier_indices_3d]\n",
    "                true_angle_binned_3d = true_angle_binned[inlier_indices_3d]\n",
    "                est_gain_binned_3d = est_gain_binned[inlier_indices_3d]\n",
    "            print(f\"Output embeddings_2d shape after nt_TDA: {embeddings_2d.shape}\")\n",
    "            print(f\"Output embeddings_3d shape after nt_TDA: {embeddings_3d.shape}\")\n",
    "            \n",
    "\n",
    "            embeddings_2d_list.append(embeddings_2d)\n",
    "            embeddings_3d_list.append(embeddings_3d)\n",
    "\n",
    "            # Fit principal curves (adapted from Chaudhuri et al.)\n",
    "            print(f\" hippocampal angle: {(hipp_angle_binned_3d[50:100] % 360)})\")\n",
    "\n",
    "            penalty_types = ['none','curvature']\n",
    "            curvature_coeffs = [1,5,10,15,20]\n",
    "\n",
    "            for penalty_type in penalty_types:\n",
    "                for curv in curvature_coeffs:\n",
    "            \n",
    "                    fit_params = {\n",
    "                        'dalpha': 0.005,\n",
    "                        'knot_order': 'nearest',\n",
    "                        'penalty_type': penalty_type, #note that curvature penalty penalizes both curvature and length * len_coeff\n",
    "                        'nKnots': 15,\n",
    "                        'curvature_coeff': curv,\n",
    "                        'len_coeff': 2,\n",
    "                        'density_coeff': 2,\n",
    "                        'delta': 0.1\n",
    "                    }\n",
    "                    # principal_curve_2d, principal_curve_2d_pre, curve_params_2d, decoded_angles_2d, mse_2d = fit_spud_to_cebra(embeddings_2d,ref_angles=hipp_angle_binned,hippocampal_angle_origin=0, session_idx=session_idx,session=session,results_save_path=results_save_path,dimension_3d=0)\n",
    "                    principal_curve_3d, principal_curve_3d_pre, curve_params_3d, decoded_angles_3d, mse_3d = fit_spud_to_cebra(embeddings_3d,ref_angles=hipp_angle_binned_3d,hippocampal_angle_origin=0, session_idx=session_idx,session=session,results_save_path=results_save_path,fit_params=fit_params,dimension_3d=1)\n",
    "\n",
    "\n",
    "                    # # Interpolate the principal curves (for calculating dists between the embedding points and princ curve)\n",
    "                    # interpolated_curve_2d = interpolate_principal_curve(principal_curve_2d, points_per_unit_distance=10)\n",
    "                    # interpolated_curve_3d = interpolate_principal_curve(principal_curve_3d, points_per_unit_distance=10)\n",
    "\n",
    "                    # Plot time vs distance from the spline to the point\n",
    "\n",
    "                    # plot_time_vs_distance(\n",
    "                    #     embeddings=embeddings_2d,\n",
    "                    #     principal_curve=interpolated_curve_2d,\n",
    "                    #     times=bins[:-1] + bin_size / 2,  x_axis_var=\"Times\",\n",
    "                    #     annotate_var=high_vel_binned, annotate_var_name=\"Velocity\",\n",
    "                    #     session=session, session_idx=session_idx,\n",
    "                    #     bin_size=bin_size,\n",
    "                    #     save_path=time_dist_save_path,\n",
    "                    #     dimension = \"2\"\n",
    "                    # )\n",
    "\n",
    "                    # plot_time_vs_distance(\n",
    "                    #     embeddings=embeddings_3d,\n",
    "                    #     principal_curve=interpolated_curve_3d,\n",
    "                    #     times=bins[:-1] + bin_size / 2,  x_axis_var=\"Times\",\n",
    "                    #     annotate_var=high_vel_binned, annotate_var_name=\"Velocity\",\n",
    "                    #     session=session, session_idx=session_idx,\n",
    "                    #     bin_size=bin_size,\n",
    "                    #     save_path=time_dist_save_path,\n",
    "                    #     dimension = \"3\"\n",
    "                    # )\n",
    "\n",
    "                    # plot_time_vs_distance(\n",
    "                    #     embeddings=embeddings_2d,\n",
    "                    #     principal_curve=interpolated_curve_2d,\n",
    "                    #     times=high_vel_binned, x_axis_var = \"Velocity)\",\n",
    "                    #     annotate_var=high_vel_binned, annotate_var_name=\"None\",\n",
    "                    #     session=session, session_idx=session_idx,\n",
    "                    #     bin_size=bin_size,\n",
    "                    #     save_path=time_dist_save_path,\n",
    "                    #     dimension = \"2\"\n",
    "                    # )\n",
    "\n",
    "                    # plot_time_vs_distance(\n",
    "                    #     embeddings=embeddings_2d,\n",
    "                    #     principal_curve=interpolated_curve_2d,\n",
    "                    #     times=high_vel_binned, x_axis_var = \"Velocity\",\n",
    "                    #     annotate_var=high_vel_binned, annotate_var_name=\"None\",\n",
    "                    #     session=session, session_idx=session_idx,\n",
    "                    #     bin_size=bin_size,\n",
    "                    #     save_path=time_dist_save_path,\n",
    "                    #     dimension = \"3\"\n",
    "                    # )\n",
    "\n",
    "                    # run_persistent_homology(\n",
    "                    #     embeddings=embeddings_2d,\n",
    "                    #     session_idx=session_idx,\n",
    "                    #     session=session,\n",
    "                    #     results_save_path=results_save_path,\n",
    "                    #     dimension=2\n",
    "                    # )\n",
    "\n",
    "                    # # Run persistent homology on 3D embeddings\n",
    "                    # run_persistent_homology(\n",
    "                    #     embeddings=embeddings_3d,\n",
    "                    #     session_idx=session_idx,\n",
    "                    #     session=session,\n",
    "                    #     results_save_path=results_save_path,\n",
    "                    #     dimension=3\n",
    "                    # )\n",
    "\n",
    "                    #UMAP and TSNE vis\n",
    "                    #umap_and_tSNE_vis(neural_data=neural_data,embeddings_2d=embeddings_2d,embeddings_3d=embeddings_3d,hipp_angle_binned=hipp_angle_binned,true_angle_binned=true_angle_binned,principal_curve_2d=principal_curve_2d,principal_curve_3d=principal_curve_3d,session=session,session_idx=session_idx,results_save_path=results_save_path)\n",
    "\n",
    "                    # Create rotating 3D plots\n",
    "                    anim_save_file = f\"{anim_save_path}/bin_size-{bin_size}/session_{session_idx}/vel_thresh_{vel_threshold}/curv_{curv}_pen_type_{penalty_type}/\"\n",
    "                    os.makedirs(anim_save_file,exist_ok=True)\n",
    "\n",
    "                    # create_rotating_3d_plot(dense_points, session, (hipp_angle_binned % 360), \"hipp angle dense\", anim_save_file, save_anim, principal_curve_3d,tt=curve_params_3d,num_labels=20)\n",
    "\n",
    "                    print(f\"curve_params shape: {curve_params_3d.shape}\")\n",
    "                    print(f\"curve_params values first 20: {curve_params_3d[:20]}\")\n",
    "\n",
    "                    print(f\"hipp_angle_binned shape: {hipp_angle_binned_3d.shape}\")\n",
    "                    print(f\"hipp_angle values first 20: {hipp_angle_binned_3d[:20]}\")\n",
    "\n",
    "                    print(f\"DIFFFFFF: {np.std(np.diff(curve_params_3d))}\")\n",
    "\n",
    "                    mean_dist = dist_tot_to_princ_curve(embeddings=embeddings_3d,principal_curve=principal_curve_3d)\n",
    "                    create_rotating_3d_plot(embeddings_3d=embeddings_3d, session=session, behav_var=(hipp_angle_binned_3d % 360), name_behav_var=\"hipp angle\", anim_save_path=anim_save_file, save_anim=save_anim, principal_curve=principal_curve_3d, tt=curve_params_3d, num_labels=20,mean_dist=mean_dist)\n",
    "                    # create_rotating_3d_plot(embeddings_3d, session, (hipp_angle_binned_3d % 360), \"hipp angle\", anim_save_file, save_anim,principal_curve=principal_curve_3d)\n",
    "                    # create_rotating_3d_plot(embeddings_3d, session, hipp_angle_binned % 360, \"hipp angle pre\", anim_save_file, save_anim, principal_curve_3d_pre)\n",
    "                    # create_rotating_3d_plot(embeddings_3d, session, true_angle_binned % 360, \"true angle\", anim_save_file, save_anim, principal_curve_3d)\n",
    "                    # create_rotating_3d_plot(embeddings_3d, session, high_vel_binned, \"velocity\", anim_save_file, save_anim, principal_curve_3d)\n",
    "                    print(f\"Created rotating 3D plots for session {session_idx}.\")\n",
    "\n",
    "\n",
    "\n",
    "            # decode_H = calculate_over_experiment_H(embeddings=embeddings_3d,tt=curve_params_3d,principal_curve=principal_curve_3d,true_angle=true_angle_binned_3d)\n",
    "\n",
    "            # plot_decode_H_vs_true_H(est_H=est_gain_binned_3d, decode_H=decode_H, session_idx=session_idx, session=session, save_path=results_save_path)\n",
    "        \n",
    "            if save_models:\n",
    "                np.save(os.path.join(model_save_path, embedding_filename_2d), embeddings_2d)\n",
    "                np.save(os.path.join(model_save_path, embedding_filename_3d), embeddings_3d)\n",
    "                print(f\"Saved embeddings: {embedding_filename_2d}, {embedding_filename_3d}\")\n",
    "\n",
    "        \n",
    "            # Collect embeddings and metadata for potential future use\n",
    "            embeddings_list.append({\n",
    "                'session_idx': session_idx + 1,\n",
    "                'rat': session.rat,\n",
    "                'day': session.day,\n",
    "                'epoch': session.epoch,\n",
    "                'bin_size': bin_size,\n",
    "                'embeddings_2d': embeddings_2d,\n",
    "                'embeddings_3d': embeddings_3d,\n",
    "                'hipp_angle_binned': hipp_angle_binned,\n",
    "                'est_gain_binned': est_gain_binned,\n",
    "                'true_angle_binned': true_angle_binned\n",
    "            })\n",
    "\n",
    "            # Optionally, save metadata to a CSV for reference\n",
    "            metadata_list.append({\n",
    "                'session_idx': session_idx + 1,\n",
    "                'rat': session.rat,\n",
    "                'day': session.day,\n",
    "                'epoch': session.epoch,\n",
    "                'bin_size': bin_size,\n",
    "                'embedding_filename_2d': embedding_filename_2d,\n",
    "                'embedding_filename_3d': embedding_filename_3d\n",
    "            })\n",
    "\n",
    "            # Save metadata after processing all sessions\n",
    "            metadata_df = pd.DataFrame(metadata_list)\n",
    "            metadata_csv_path = os.path.join(results_save_path, 'embeddings_metadata.csv')\n",
    "            metadata_df.to_csv(metadata_csv_path, index=False)\n",
    "            print(f\"Saved embeddings metadata to {metadata_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1673, 2091]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 36\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# embeddings_2d = apply_cebra(neural_data,2)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# embeddings_3d = apply_cebra(neural_data,3)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Setting a random state for reproducibility\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets for hipp_angle\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m train_emb_2d_hipp, test_emb_2d_hipp, train_labels_hipp, test_labels_hipp \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhipp_angle_binned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m train_emb_3d_hipp, test_emb_3d_hipp, train_labels_hipp_3d, test_labels_hipp_3d \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     41\u001b[0m     embeddings_3d, hipp_angle_binned, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets for true_angle\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NRSC510/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NRSC510/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2777\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2777\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2779\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2780\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2781\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2782\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NRSC510/lib/python3.9/site-packages/sklearn/utils/validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NRSC510/lib/python3.9/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1673, 2091]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "hipp_angle_binned = hipp_angle_binned % 360\n",
    "true_angle_binned = true_angle_binned % 360\n",
    "shuffled_est_gain_binned = np.random.permutation(est_gain_binned)\n",
    "shuffled_hipp_angle_binned = np.random.permutation(hipp_angle_binned)\n",
    "shuffled_true_angle_binned = np.random.permutation(true_angle_binned)\n",
    "\n",
    "\n",
    "def evaluate_decoding(true, pred):\n",
    "    r2 = r2_score(true, pred)\n",
    "    mse = mean_squared_error(true, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return r2, rmse\n",
    "\n",
    "\n",
    "# make sure variables are defined and raise error if not\n",
    "required_vars = ['embeddings_2d', 'embeddings_3d', 'hipp_angle_binned', 'est_gain_binned']\n",
    "for var in required_vars:\n",
    "    if var not in globals():\n",
    "        raise ValueError(f\"Variable '{var}' is not defined.\")\n",
    "    \n",
    "\n",
    "# embeddings_2d = apply_cebra(neural_data,2)\n",
    "# embeddings_3d = apply_cebra(neural_data,3)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# Using 80% for training and 20% for testing\n",
    "# Setting a random state for reproducibility\n",
    "# Split the data into training and testing sets for hipp_angle\n",
    "\n",
    "train_emb_2d_hipp, test_emb_2d_hipp, train_labels_hipp, test_labels_hipp = train_test_split(\n",
    "    embeddings_2d, hipp_angle_binned, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_emb_3d_hipp, test_emb_3d_hipp, train_labels_hipp_3d, test_labels_hipp_3d = train_test_split(\n",
    "    embeddings_3d, hipp_angle_binned, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets for true_angle\n",
    "train_emb_2d_true, test_emb_2d_true, train_labels_true, test_labels_true = train_test_split(\n",
    "    embeddings_2d, true_angle_binned, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_emb_3d_true, test_emb_3d_true, train_labels_true_3d, test_labels_true_3d = train_test_split(\n",
    "    embeddings_3d, true_angle_binned, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets for shuffled hipp_angle\n",
    "train_emb_2d_shuffled_hipp, test_emb_2d_shuffled_hipp, train_labels_shuffled_hipp, test_labels_shuffled_hipp = train_test_split(\n",
    "    embeddings_2d, shuffled_hipp_angle_binned, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_emb_3d_shuffled_hipp, test_emb_3d_shuffled_hipp, train_labels_shuffled_hipp_3d, test_labels_shuffled_hipp_3d = train_test_split(\n",
    "    embeddings_3d, shuffled_hipp_angle_binned, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets for shuffled true_angle\n",
    "train_emb_2d_shuffled_true, test_emb_2d_shuffled_true, train_labels_shuffled_true, test_labels_shuffled_true = train_test_split(\n",
    "    embeddings_2d, shuffled_true_angle_binned, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_emb_3d_shuffled_true, test_emb_3d_shuffled_true, train_labels_shuffled_true_3d, test_labels_shuffled_true_3d = train_test_split(\n",
    "    embeddings_3d, shuffled_true_angle_binned, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize kNN Decoders for estGain and hippAngle\n",
    "knn_est_gain_2d = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "knn_hipp_angle_2d = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "\n",
    "knn_est_gain_3d = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "knn_hipp_angle_3d = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "\n",
    "#shuffled hipp\n",
    "\n",
    "knn_est_gain_2d_shuffled = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "knn_hipp_angle_2d_shuffled = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "\n",
    "knn_est_gain_3d_shuffled = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "knn_hipp_angle_3d_shuffled = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "\n",
    "# Initialize kNN Decoders for true_angle\n",
    "knn_true_angle_2d = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "knn_true_angle_3d = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "\n",
    "# Shuffled decoders for true_angle\n",
    "knn_true_angle_2d_shuffled = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "knn_true_angle_3d_shuffled = cebra.KNNDecoder(n_neighbors=50, metric='cosine')\n",
    "\n",
    "\n",
    "# Train decoders on 2D embeddings for estGain and hippAngle\n",
    "knn_est_gain_2d.fit(train_emb_2d_hipp, train_labels_hipp)     # estGain\n",
    "knn_hipp_angle_2d.fit(train_emb_2d_hipp, train_labels_hipp)   # hippAngle\n",
    "\n",
    "# Train decoders on 3D embeddings for estGain and hippAngle\n",
    "knn_est_gain_3d.fit(train_emb_3d_hipp, train_labels_hipp_3d)     # estGain\n",
    "knn_hipp_angle_3d.fit(train_emb_3d_hipp, train_labels_hipp_3d)   # hippAngle\n",
    "\n",
    "# Train decoders on 2D embeddings for shuffled hippAngle\n",
    "knn_est_gain_2d_shuffled.fit(train_emb_2d_shuffled_hipp, train_labels_shuffled_hipp)     # estGain\n",
    "knn_hipp_angle_2d_shuffled.fit(train_emb_2d_shuffled_hipp, train_labels_shuffled_hipp)   # hippAngle\n",
    "\n",
    "# Train decoders on 3D embeddings for shuffled hippAngle\n",
    "knn_est_gain_3d_shuffled.fit(train_emb_3d_shuffled_hipp, train_labels_shuffled_hipp_3d)     # estGain\n",
    "knn_hipp_angle_3d_shuffled.fit(train_emb_3d_shuffled_hipp, train_labels_shuffled_hipp_3d)   # hippAngle\n",
    "\n",
    "# Train decoders on 2D and 3D embeddings for true_angle\n",
    "knn_true_angle_2d.fit(train_emb_2d_true, train_labels_true)  # true_angle 2D\n",
    "knn_true_angle_3d.fit(train_emb_3d_true, train_labels_true_3d)  # true_angle 3D\n",
    "\n",
    "# train Shuffled data training for true_angle\n",
    "knn_true_angle_2d_shuffled.fit(train_emb_2d_shuffled_true, train_labels_shuffled_true)  # true_angle 2D Shuffled\n",
    "knn_true_angle_3d_shuffled.fit(train_emb_3d_shuffled_true, train_labels_shuffled_true_3d)  # true_angle 3D Shuffled\n",
    "\n",
    "\n",
    "# Predict on 2D test embeddings for estGain and hippAngle\n",
    "est_gain_pred_2d = knn_est_gain_2d.predict(test_emb_2d_hipp)\n",
    "hipp_angle_pred_2d = knn_hipp_angle_2d.predict(test_emb_2d_hipp)\n",
    "\n",
    "# Predict on 3D test embeddings for estGain and hippAngle\n",
    "est_gain_pred_3d = knn_est_gain_3d.predict(test_emb_3d_hipp)\n",
    "hipp_angle_pred_3d = knn_hipp_angle_3d.predict(test_emb_3d_hipp)\n",
    "\n",
    "# Predict on 2D test embeddings for shuffled hippAngle\n",
    "est_gain_pred_2d_shuffled = knn_est_gain_2d_shuffled.predict(test_emb_2d_shuffled_hipp)\n",
    "hipp_angle_pred_2d_shuffled = knn_hipp_angle_2d_shuffled.predict(test_emb_2d_shuffled_hipp)\n",
    "\n",
    "# Predict on 3D test embeddings for shuffled hippAngle\n",
    "est_gain_pred_3d_shuffled = knn_est_gain_3d_shuffled.predict(test_emb_3d_shuffled_hipp)\n",
    "hipp_angle_pred_3d_shuffled = knn_hipp_angle_3d_shuffled.predict(test_emb_3d_shuffled_hipp)\n",
    "\n",
    "# Predictions on 2D and 3D test embeddings for true_angle\n",
    "true_angle_pred_2d = knn_true_angle_2d.predict(test_emb_2d_true)\n",
    "true_angle_pred_3d = knn_true_angle_3d.predict(test_emb_3d_true)\n",
    "\n",
    "# Shuffled predictions for true_angle on 2D and 3D test embeddings\n",
    "true_angle_pred_2d_shuffled = knn_true_angle_2d_shuffled.predict(test_emb_2d_shuffled_true)\n",
    "true_angle_pred_3d_shuffled = knn_true_angle_3d_shuffled.predict(test_emb_3d_shuffled_true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"test labels true{test_labels_true[:10]}\")\n",
    "print(f\"test labels hipp{test_labels_hipp[:10]}\")\n",
    "print(f\"hipp angle pred {hipp_angle_pred_2d[:10]}\")\n",
    "# 2D Embeddings Evaluation\n",
    "# 2D Embeddings Evaluation for estGain and hippAngle\n",
    "r2_est_2d, rmse_est_2d = evaluate_decoding(test_labels_hipp, est_gain_pred_2d)\n",
    "r2_hipp_2d, rmse_hipp_2d = evaluate_decoding(test_labels_hipp, hipp_angle_pred_2d)\n",
    "\n",
    "# 3D Embeddings Evaluation for estGain and hippAngle\n",
    "r2_est_3d, rmse_est_3d = evaluate_decoding(test_labels_hipp_3d, est_gain_pred_3d)\n",
    "r2_hipp_3d, rmse_hipp_3d = evaluate_decoding(test_labels_hipp_3d, hipp_angle_pred_3d)\n",
    "\n",
    "# 2D Embeddings Evaluation for shuffled hippAngle\n",
    "r2_est_2d_shuffled, rmse_est_2d_shuffled = evaluate_decoding(test_labels_shuffled_hipp, est_gain_pred_2d_shuffled)\n",
    "r2_hipp_2d_shuffled, rmse_hipp_2d_shuffled = evaluate_decoding(test_labels_shuffled_hipp, hipp_angle_pred_2d_shuffled)\n",
    "\n",
    "# 3D Embeddings Evaluation for shuffled hippAngle\n",
    "r2_est_3d_shuffled, rmse_est_3d_shuffled = evaluate_decoding(test_labels_shuffled_hipp_3d, est_gain_pred_3d_shuffled)\n",
    "r2_hipp_3d_shuffled, rmse_hipp_3d_shuffled = evaluate_decoding(test_labels_shuffled_hipp_3d, hipp_angle_pred_3d_shuffled)\n",
    "\n",
    "# 2D and 3D Embeddings Evaluation for true_angle\n",
    "r2_true_2d, rmse_true_2d = evaluate_decoding(test_labels_true, true_angle_pred_2d)\n",
    "r2_true_3d, rmse_true_3d = evaluate_decoding(test_labels_true_3d, true_angle_pred_3d)\n",
    "\n",
    "# Shuffled data evaluation for true_angle\n",
    "r2_true_2d_shuffled, rmse_true_2d_shuffled = evaluate_decoding(test_labels_shuffled_true, true_angle_pred_2d_shuffled)\n",
    "r2_true_3d_shuffled, rmse_true_3d_shuffled = evaluate_decoding(test_labels_shuffled_true_3d, true_angle_pred_3d_shuffled)\n",
    "\n",
    "\n",
    "\n",
    "# Vis of decoding errors\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar Plot for RMSE \n",
    "rmse_values = [rmse_est_2d, rmse_hipp_2d, rmse_true_2d, rmse_est_3d, rmse_hipp_3d, rmse_true_3d,\n",
    "               rmse_est_2d_shuffled, rmse_hipp_2d_shuffled, rmse_true_2d_shuffled, rmse_est_3d_shuffled, rmse_hipp_3d_shuffled, rmse_true_3d_shuffled]\n",
    "labels_bar = ['estGain 2D', 'hippAngle 2D', 'trueAngle 2D', 'estGain 3D', 'hippAngle 3D', 'trueAngle 3D',\n",
    "              'estGain 2D Shuffled', 'hippAngle 2D Shuffled', 'trueAngle 2D Shuffled', 'estGain 3D Shuffled', 'hippAngle 3D Shuffled', 'trueAngle 3D Shuffled']\n",
    "colors_bar = ['skyblue', 'orange', 'green', 'skyblue', 'orange', 'green', 'gray', 'gray', 'gray', 'gray', 'gray', 'gray']\n",
    "\n",
    "ax[0].bar(labels_bar, rmse_values, color=colors_bar)\n",
    "ax[0].set_ylabel('Root Mean Squared Error (RMSE)')\n",
    "ax[0].set_title('Decoding Root Mean Squared Error (Including true_angle and Shuffled Data)')\n",
    "ax[0].set_xticklabels(labels_bar, rotation=45, ha='right')\n",
    "\n",
    "# Scatter Plot of R2 Score vs rmse\n",
    "ax[1].scatter(rmse_est_2d, r2_est_2d, label='estGain 2D', color='skyblue', s=100)\n",
    "ax[1].scatter(rmse_hipp_2d, r2_hipp_2d, label='hippAngle 2D', color='orange', s=100)\n",
    "ax[1].scatter(rmse_true_2d, r2_true_2d, label='trueAngle 2D', color='green', s=100)\n",
    "ax[1].scatter(rmse_est_3d, r2_est_3d, label='estGain 3D', color='skyblue', alpha=0.6, s=100)\n",
    "ax[1].scatter(rmse_hipp_3d, r2_hipp_3d, label='hippAngle 3D', color='orange', alpha=0.6, s=100)\n",
    "ax[1].scatter(rmse_true_3d, r2_true_3d, label='trueAngle 3D', color='green', alpha=0.6, s=100)\n",
    "ax[1].set_xlabel('Root Mean Squared Error (RMSE)')\n",
    "ax[1].set_ylabel('R² Score')\n",
    "ax[1].set_title('Decoding Performance Comparison (Including true_angle and Shuffled Data)')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a summary table of decoding performance\n",
    "data = {\n",
    "    'Embedding Dimension': ['2D', '2D', '2D', '3D', '3D', '3D', '2D Shuffled', '2D Shuffled', '2D Shuffled', '3D Shuffled', '3D Shuffled', '3D Shuffled'],\n",
    "    'Variable': ['estGain', 'hippAngle', 'trueAngle', 'estGain', 'hippAngle', 'trueAngle', \n",
    "                 'estGain', 'hippAngle', 'trueAngle', 'estGain', 'hippAngle', 'trueAngle'],\n",
    "    'R² Score': [r2_est_2d, r2_hipp_2d, r2_true_2d, r2_est_3d, r2_hipp_3d, r2_true_3d, \n",
    "                 r2_est_2d_shuffled, r2_hipp_2d_shuffled, r2_true_2d_shuffled, r2_est_3d_shuffled, r2_hipp_3d_shuffled, r2_true_3d_shuffled],\n",
    "    'RMSE': [rmse_est_2d, rmse_hipp_2d, rmse_true_2d, rmse_est_3d, rmse_hipp_3d, rmse_true_3d, \n",
    "             rmse_est_2d_shuffled, rmse_hipp_2d_shuffled, rmse_true_2d_shuffled, rmse_est_3d_shuffled, rmse_hipp_3d_shuffled, rmse_true_3d_shuffled]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n",
    "\n",
    "#save the stats\n",
    "# df.to_csv('/decoding_results.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NRSC510",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
