{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "sys.path.append(\"/Users/devenshidfar/Desktop/Masters/NRSC_510B/cebra_control_recal\")\n",
    "import old_jupyter_notebook_version.cebra_utils as cebra_utils\n",
    "from old_jupyter_notebook_version.cebra_utils import *\n",
    "import cebra\n",
    "importlib.reload(cebra_utils)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import pickle  # For saving filtered expt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_path = os.path.join('/Users/devenshidfar/Desktop/Masters/NRSC_510B/'\n",
    "                         'cebra_control_recal/mat_code_and_data/',\n",
    "                         'data/NN_opticflow_dataset.mat')\n",
    "data = scipy.io.loadmat(file_path, squeeze_me=True, struct_as_record=False)\n",
    "expt_optic_flow = data['expt']\n",
    "\n",
    "file_path = os.path.join('/Users/devenshidfar/Desktop/Masters/NRSC_510B/',\n",
    "                         'cebra_control_recal/mat_code_and_data/'\n",
    "                         'data/expt_landmark.mat')\n",
    "data = scipy.io.loadmat(file_path, squeeze_me=True, struct_as_record=False)\n",
    "expt_landmark = data['expt']\n",
    "\n",
    "print(expt_optic_flow.shape)\n",
    "\n",
    "print(f\"Experiment data type: {expt_optic_flow.dtype}\")\n",
    "print(f\"Experiment data shape: {expt_optic_flow.shape}\")\n",
    "\n",
    "print(expt_landmark.shape)\n",
    "\n",
    "print(f\"Experiment data type: {expt_landmark.dtype}\")\n",
    "print(f\"Experiment data shape: {expt_landmark.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THINGS TO DO:\n",
    "# take a fourier transform of the H decode to find strongest frequencies\n",
    "#fix the avg angle diff calculation\n",
    "#normalize cebra data to be on unit sphere/see if then it lives on the unit sphere\n",
    "#plot scatter of SI score vs num_clusters used\n",
    "\n",
    "# Config Parameters\n",
    "session_choose = True\n",
    "\n",
    "if(session_choose == True):\n",
    "    landmark_sessions = []\n",
    "    optic_flow_sessions = []\n",
    "else:\n",
    "    landmark_num_trials = 20\n",
    "    landmark_control_point = 5\n",
    "\n",
    "    optic_flow_num_trials = 14\n",
    "    optic_flow_control_point = 14\n",
    "\n",
    "\n",
    "\n",
    "max_num_reruns = 1\n",
    "\n",
    "expts = [\n",
    "    (\"optic_flow\", expt_optic_flow, optic_flow_control_point, optic_flow_num_trials),\n",
    "    (\"landmark\", expt_landmark, landmark_control_point, landmark_num_trials)\n",
    "]\n",
    "\n",
    "save_folder = 'neural_save'\n",
    "\n",
    "for expt_name, expt, control_point, num_trials in expts:\n",
    "\n",
    "\n",
    "    all_sessions_data = []\n",
    "    all_neural_data = []\n",
    "    all_embeddings_3d = []\n",
    "    all_principal_curves_3d = []\n",
    "    all_curve_params_3d = []\n",
    "    all_binned_hipp_angle = []\n",
    "    all_binned_true_angle = []\n",
    "    all_binned_est_gain = []\n",
    "    all_binned_high_vel = []\n",
    "    all_decoded_angles = []\n",
    "    all_filtered_decoded_angles_unwrap = []\n",
    "    all_decode_H = []\n",
    "    all_session_idx = []\n",
    "    all_rat = []\n",
    "    all_day = []\n",
    "    all_epoch = []\n",
    "    all_num_skipped_clusters = []\n",
    "    all_num_used_clusters = []\n",
    "    all_avg_skipped_cluster_isolation_quality = []\n",
    "    all_avg_used_cluster_isolation_quality = []\n",
    "    all_mean_distance_to_principal_curve = []\n",
    "    all_mean_angle_difference = []\n",
    "    all_shuffled_mean_angle_difference = []\n",
    "    all_SI_score_hipp = []\n",
    "    all_SI_score_true = []\n",
    "    all_SI_score_high_dim = []\n",
    "    all_mse_decode_vs_true = []\n",
    "    all_mean_H_difference = []\n",
    "    all_std_H_difference = []\n",
    "\n",
    "    control_count = 0\n",
    "\n",
    "    model_save_path = os.path.join('/Users/devenshidfar/Desktop/Masters/'\n",
    "                                   'NRSC_510B/cebra_control_recal/models')\n",
    "\n",
    "    # Set to 1 to save the models and animations\n",
    "    save_models = 1\n",
    "    save_anim = 1\n",
    "    load_npy = 0\n",
    "    rm_outliers = True\n",
    "\n",
    "    # Define bin sizes (in seconds)\n",
    "    bin_sizes = [1]  # will loop through how ever many bin sizes there are\n",
    "\n",
    "    # Initialize lists to store embeddings and metadata\n",
    "    embeddings_list = []\n",
    "    metadata_list = []\n",
    "\n",
    "    # Define velocity threshold\n",
    "    vel_threshold = 5  # degrees per second\n",
    "\n",
    "    # Main Processing Loop\n",
    "    for bin_size in bin_sizes:\n",
    "        print(f\"\\nProcessing bin_size: {bin_size} second(s)\")\n",
    "\n",
    "        csv_dict_all_data = []\n",
    "\n",
    "        for session_idx, session in enumerate(expt):\n",
    "            control_count += 1\n",
    "\n",
    "\n",
    "            # Control Session Skipping\n",
    "            if control_count <= control_point:\n",
    "                print(f\"Skipping session {session_idx + 1}\")\n",
    "                continue \n",
    "            elif control_count > control_point + num_trials:\n",
    "                print(\"Reached the desired number of trials.\" \n",
    "                      \"Exiting session loop.\")\n",
    "                break\n",
    "\n",
    "            print(f\"\\nProcessing session {session_idx + 1}/{len(expt)}\")\n",
    "            print(\n",
    "                  f\"Rat: {session.rat}, \"\n",
    "                  \"Day: {session.day}, \"\n",
    "                  \"Epoch: {session.epoch}\"\n",
    "            )\n",
    "\n",
    "    \n",
    "            ros_data = session.rosdata\n",
    "\n",
    "            start_time = ros_data.startTs\n",
    "            end_time = ros_data.stopTs\n",
    "\n",
    "            # Convert to seconds (originally in milliseconds)\n",
    "            enc_times = np.array(ros_data.encTimes - start_time) / 1e6  \n",
    "            vel = np.array(ros_data.vel)\n",
    "\n",
    "            # Ensure enc_times and vel are valid\n",
    "            valid_idx = np.isfinite(enc_times) & np.isfinite(vel)\n",
    "            enc_times = enc_times[valid_idx]\n",
    "            vel = vel[valid_idx]\n",
    "\n",
    "            # Get indices where vel > vel_threshold\n",
    "            high_vel_idx = vel > vel_threshold\n",
    "\n",
    "            if np.sum(high_vel_idx) == 0:\n",
    "                print(\"No data points where vel > vel_threshold. \"\n",
    "                      \"Skipping session.\")\n",
    "                continue\n",
    "\n",
    "            # Filter enc_times and behavioral variables\n",
    "            enc_times_high_vel = enc_times[high_vel_idx]\n",
    "            high_vel_filtered = vel[high_vel_idx]\n",
    "            est_gain_filtered = ( \n",
    "                np.array(ros_data.estGain)[valid_idx][high_vel_idx]\n",
    "            )\n",
    "            print(f\"est_gain filtered: {est_gain_filtered[100:130]}\")\n",
    "            hipp_angle_filtered = (\n",
    "                np.array(ros_data.hippAngle)[valid_idx][high_vel_idx]\n",
    "                )\n",
    "            true_angle_filtered = (\n",
    "                np.array(ros_data.encAngle)[valid_idx][high_vel_idx]\n",
    "                )\n",
    "            rel_angle_filtered = (\n",
    "                np.array(ros_data.relAngle)[valid_idx][high_vel_idx]\n",
    "                )\n",
    "\n",
    "            # Define bins over enc_times_high_vel (Non-Overlapping)\n",
    "            bins = np.arange(\n",
    "                enc_times_high_vel[0], \n",
    "                enc_times_high_vel[-1] + bin_size, bin_size\n",
    "            )\n",
    "\n",
    "            if len(bins) < 2:\n",
    "                print(\"Not enough data after filtering for high velocity. \"\n",
    "                      \"Skipping session.\")\n",
    "                continue\n",
    "\n",
    "            # Bin behavioral variables using non-overlapping bins\n",
    "            binned_est_gain, _, _ = stats.binned_statistic(\n",
    "                enc_times_high_vel, \n",
    "                est_gain_filtered, \n",
    "                statistic='mean', \n",
    "                bins=bins\n",
    "            )\n",
    "            binned_hipp_angle, _, _ = stats.binned_statistic(\n",
    "                enc_times_high_vel, \n",
    "                hipp_angle_filtered, \n",
    "                statistic='mean', \n",
    "                bins=bins\n",
    "            )\n",
    "            binned_true_angle, _, _ = stats.binned_statistic(\n",
    "                enc_times_high_vel, \n",
    "                true_angle_filtered, \n",
    "                statistic='mean', \n",
    "                bins=bins\n",
    "            )\n",
    "            binned_high_vel, _, _ = stats.binned_statistic(\n",
    "                enc_times_high_vel, \n",
    "                high_vel_filtered, \n",
    "                statistic='mean', \n",
    "                bins=bins\n",
    "            )\n",
    "            binned_rel_angle, _, _ = stats.binned_statistic(\n",
    "                enc_times_high_vel, \n",
    "                rel_angle_filtered, \n",
    "                statistic='mean', \n",
    "                bins=bins\n",
    "            )\n",
    "\n",
    "            # Remove NaN bins\n",
    "            valid_bins = (\n",
    "                ~np.isnan(binned_hipp_angle) \n",
    "                & ~np.isnan(binned_true_angle) \n",
    "                & ~np.isnan(binned_est_gain) \n",
    "                & ~np.isnan(binned_high_vel)\n",
    "            )\n",
    "            if not np.all(valid_bins):\n",
    "                print(f\"Removing {np.sum(~valid_bins)} bins with NaNs.\")\n",
    "                binned_hipp_angle = binned_hipp_angle[valid_bins]\n",
    "                binned_true_angle = binned_true_angle[valid_bins]\n",
    "                binned_est_gain = binned_est_gain[valid_bins]\n",
    "                binned_high_vel = binned_high_vel[valid_bins]\n",
    "                binned_rel_angle = binned_rel_angle[valid_bins]\n",
    "                # Adjust bins to match the number of valid bins\n",
    "                bins = bins[:-1][valid_bins]  \n",
    "\n",
    "            # Filter Spike Times Based on High Velocity\n",
    "            all_spikes = []\n",
    "            skipped_clusters = 0\n",
    "            used_clusters = 0\n",
    "            used_cluster_isolation_quality_list = []\n",
    "            skipped_cluster_isolation_quality_list = []\n",
    "\n",
    "            for cluster in session.clust:\n",
    "                if cluster.isolationQuality > 4:\n",
    "                    # done in Madhav et al., 2024\n",
    "                    print(f\"Skipping cluster {cluster.name}\"\n",
    "                    \"due to low isolation quality ({cluster.isolationQuality})\") \n",
    "                    skipped_clusters += 1\n",
    "                    skipped_cluster_isolation_quality_list.append(cluster.isolationQuality)\n",
    "                    continue\n",
    "                else:\n",
    "                    used_clusters += 1\n",
    "                    used_cluster_isolation_quality_list.append(cluster.isolationQuality)\n",
    "\n",
    "                # Convert to seconds\n",
    "                spike_times_sec = (cluster.ts - start_time) / 1e6  \n",
    "                vel_at_spikes = cluster.vel\n",
    "\n",
    "                # Include spikes where vel_at_spikes > vel_threshold\n",
    "                include_spikes = vel_at_spikes > vel_threshold\n",
    "                spike_times_sec_high_vel = spike_times_sec[include_spikes]\n",
    "\n",
    "                if len(spike_times_sec_high_vel) == 0:\n",
    "                    print(f\"No spikes for cluster {cluster.name}\" \n",
    "                        \"at high velocities. Skipping cluster.\")\n",
    "                    continue\n",
    "\n",
    "                # Bin spikes using non-overlapping bins\n",
    "                binned_spikes, _, _ = stats.binned_statistic(\n",
    "                    spike_times_sec_high_vel,\n",
    "                    np.ones_like(spike_times_sec_high_vel),\n",
    "                    statistic='sum',\n",
    "                    bins=bins\n",
    "                )\n",
    "                # Note: In non-overlapping bins, \n",
    "                # we don't assign NaNs as in overlapping bins\n",
    "\n",
    "                # Append to all_spikes\n",
    "                all_spikes.append(binned_spikes)\n",
    "\n",
    "            #Cluster statistics\n",
    "\n",
    "            used_cluster_isolation_quality = np.asarray(used_cluster_isolation_quality_list)\n",
    "            skipped_cluster_isolation_quality = np.asarray(skipped_cluster_isolation_quality_list)\n",
    "\n",
    "            num_skipped_cluster = len(skipped_cluster_isolation_quality)\n",
    "            num_used_cluster = len(used_cluster_isolation_quality)\n",
    "\n",
    "            avg_skipped_cluster_isolation_quality = np.mean(skipped_cluster_isolation_quality)\n",
    "            avg_used_cluster_isolation_quality = np.mean(used_cluster_isolation_quality)\n",
    "\n",
    "            if not all_spikes:\n",
    "                print(\"No valid spike data after filtering. Skipping session.\")\n",
    "                continue\n",
    "\n",
    "            # Assemble neural data: shape (num_bins, num_clusters)\n",
    "            neural_data = np.array(all_spikes).T  # Shape: (num_bins, num_clusters)\n",
    "\n",
    "            # Consistency Check Between Bins\n",
    "            num_bins_neural = neural_data.shape[0]\n",
    "            num_bins_behavior = len(binned_est_gain)\n",
    "\n",
    "            if num_bins_neural != num_bins_behavior:\n",
    "                print(f\"Warning: Number of neural data bins ({num_bins_neural})\" \n",
    "                    \"does not match behavioral data bins ({num_bins_behavior}).\" \n",
    "                    \"Adjusting to minimum.\")\n",
    "                min_bins = min(num_bins_neural, num_bins_behavior)\n",
    "                neural_data = neural_data[:min_bins, :]\n",
    "                binned_est_gain = binned_est_gain[:min_bins]\n",
    "                binned_hipp_angle = binned_hipp_angle[:min_bins]\n",
    "                binned_true_angle = binned_true_angle[:min_bins]\n",
    "                binned_high_vel = binned_high_vel[:min_bins]\n",
    "                bins = bins[:min_bins]\n",
    "\n",
    "            binned_est_gain_temp = binned_est_gain\n",
    "            binned_hipp_angle_temp = binned_hipp_angle\n",
    "            binned_true_angle_temp = binned_true_angle\n",
    "            binned_high_vel_temp = binned_high_vel\n",
    "\n",
    "            embeddings_2d_list = []\n",
    "            embeddings_3d_list = []\n",
    "\n",
    "            embedding_filename_2d = f\"bin_size-{bin_size}_embeddings_2d_rat\"\n",
    "            \"{session.rat}_day{session.day}_epoch{session.epoch}.npy\"\n",
    "            embedding_filename_3d = f\"bin_size-{bin_size}_embeddings_3d_rat\"\n",
    "            \"{session.rat}_day{session.day}_epoch{session.epoch}.npy\"\n",
    "\n",
    "\n",
    "            compile_path = f'/Users/devenshidfar/Desktop/Masters/NRSC_510B/'\n",
    "            'cebra_control_recal/results/{expt_name}/{save_folder}'\n",
    "            os.makedirs(compile_path,exist_ok=True)\n",
    "\n",
    "            temperature = [1]  \n",
    "            for temp in temperature:\n",
    "    \n",
    "                binned_est_gain = binned_est_gain_temp\n",
    "                binned_hipp_angle = binned_hipp_angle_temp\n",
    "                binned_true_angle = binned_true_angle_temp\n",
    "                binned_high_vel = binned_high_vel_temp\n",
    "\n",
    "\n",
    "                base_path = f'/Users/devenshidfar/Desktop/Masters/NRSC_510B/'\n",
    "                'cebra_control_recal/results/{save_folder}/{expt_name}/'\n",
    "                'temperature_{temp}'\n",
    "                results_save_path = f'{base_path}/rat_{session.rat}/'\n",
    "                'session_{session_idx}'\n",
    "                anim_save_path = f'{results_save_path}/3d_animations'\n",
    "                time_dist_save_path = f'{results_save_path}/time_dist_plot'\n",
    "                SI_save_path = f'{results_save_path}/SI_Plots'\n",
    "                param_plot_path = f'{results_save_path}/Param_Plots'\n",
    "                behav_var_plot_path = f'{results_save_path}/behav_var_plot'\n",
    "                csv_save_path = f'{results_save_path}/csv_file'\n",
    "                spectrogram_path = f'{results_save_path}/spatial_spectrograms'\n",
    "\n",
    "                os.makedirs(results_save_path,exist_ok=True)\n",
    "\n",
    "                pdf_filename = os.path.join(results_save_path, f'session_{session_idx}.pdf')\n",
    "                pdf = PdfPages(pdf_filename)\n",
    "\n",
    "                # Create necessary directories\n",
    "                os.makedirs(model_save_path, exist_ok=True)\n",
    "                os.makedirs(anim_save_path, exist_ok=True)\n",
    "                os.makedirs(SI_save_path, exist_ok=True)\n",
    "                os.makedirs(param_plot_path, exist_ok=True)\n",
    "                os.makedirs(csv_save_path, exist_ok=True)\n",
    "                os.makedirs(spectrogram_path, exist_ok=True)\n",
    "\n",
    "                # Apply CEBRA embeddings\n",
    "                # embeddings_2d = apply_cebra(neural_data, 2,temperature=temp)\n",
    "                # embeddings_3d = apply_cebra(neural_data, 3,temperature=temp)\n",
    "\n",
    "                num_reruns = 0\n",
    "                for num_reruns in range(max_num_reruns):\n",
    "\n",
    "                    def apply_cebra(neural_data=None,output_dimension=3,temperature=1):\n",
    "\n",
    "                        ''' default hyper-params for CEBRA model\n",
    "\n",
    "                        # Model Architecture (model_architecture): 'offset10-model'\n",
    "                        # Batch Size (batch_size): 512\n",
    "                        # Temperature Mode (temperature_mode): \"auto\"\n",
    "                        # Learning Rate (learning_rate): 0.001\n",
    "                        # Max Iterations (max_iterations): 10,000\n",
    "                        # Time Offsets (time_offsets): 10\n",
    "                        # Output Dimension (output_dimension): 8\n",
    "                        # Device (device): \"cuda_if_available\" (falls back to \"cpu\" if no GPU is available)\n",
    "                        # Verbose (verbose): False\n",
    "\n",
    "                        '''\n",
    "                        \n",
    "                        model = cebra.CEBRA(\n",
    "                            output_dimension=output_dimension, \n",
    "                            max_iterations=1000, \n",
    "                            batch_size = 512,\n",
    "                            temperature=temperature,)   \n",
    "                        model.fit(neural_data)\n",
    "                        embeddings = model.transform(neural_data)\n",
    "                        return embeddings\n",
    "                    \n",
    "                    # Shape: (num_bins, num_clusters)\n",
    "                    neural_data = np.array(all_spikes).T \n",
    "                    embeddings_high_dim = apply_cebra(neural_data, 3, temperature=temp)\n",
    "\n",
    "                    # from sklearn.manifold import Isomap\n",
    "\n",
    "                    # isomap = Isomap(n_components=3)\n",
    "                    # embeddings_3d = isomap.fit_transform(embeddings_high_dim)\n",
    "\n",
    "                    embeddings_3d = embeddings_high_dim\n",
    "\n",
    "                    print(f\"Output embeddings_3d shape before nt_TDA and mask: {embeddings_3d.shape}\")\n",
    "\n",
    "                    # Create NaN mask based on behavioral variables and embeddings\n",
    "                    nan_mask_3d = (\n",
    "                        ~np.isnan(embeddings_3d).any(axis=1) &\n",
    "                        ~np.isnan(binned_hipp_angle) &\n",
    "                        ~np.isnan(binned_true_angle) &\n",
    "                        ~np.isnan(binned_est_gain) &\n",
    "                        ~np.isnan(binned_high_vel)\n",
    "                    )\n",
    "\n",
    "                    # embeddings_2d = embeddings_2d[nan_mask_2d, :]\n",
    "                    embeddings_3d = embeddings_3d[nan_mask_3d, :]\n",
    "                    binned_hipp_angle = binned_hipp_angle[nan_mask_3d]\n",
    "                    binned_true_angle = binned_true_angle[nan_mask_3d]\n",
    "                    binned_est_gain = binned_est_gain[nan_mask_3d]\n",
    "                    binned_high_vel = binned_high_vel[nan_mask_3d]\n",
    "\n",
    "                    nan_mask_high_dim = (\n",
    "                        ~np.isnan(embeddings_high_dim).any(axis=1) &\n",
    "                        ~np.isnan(binned_hipp_angle) &\n",
    "                        ~np.isnan(binned_true_angle) &\n",
    "                        ~np.isnan(binned_est_gain) &\n",
    "                        ~np.isnan(binned_high_vel)\n",
    "                    )\n",
    "\n",
    "                    #for high dim\n",
    "                    embeddings_high_dim = embeddings_high_dim[nan_mask_high_dim, :]\n",
    "                    binned_hipp_angle_high_dim = binned_hipp_angle[nan_mask_high_dim]\n",
    "\n",
    "                    # print(f\"Output embeddings_2d shape after NaN mask: {embeddings_2d.shape}\")\n",
    "                    print(f\"Output embeddings_3d shape after NaN mask: {embeddings_3d.shape}\")\n",
    "\n",
    "                    if rm_outliers:\n",
    "                        print(\"Removing outliers...\")\n",
    "                        #inlier_indices_2d = nt_TDA(embeddings_2d)\n",
    "                        inlier_indices_3d = nt_TDA(embeddings_3d)\n",
    "                        inlier_indices_high_dim = nt_TDA(embeddings_high_dim)\n",
    "                        #embeddings_2d = embeddings_2d[inlier_indices_2d, :]\n",
    "                        embeddings_3d = embeddings_3d[inlier_indices_3d, :]\n",
    "                        binned_hipp_angle = binned_hipp_angle[inlier_indices_3d]\n",
    "                        binned_true_angle = binned_true_angle[inlier_indices_3d]\n",
    "                        binned_est_gain = binned_est_gain[inlier_indices_3d]\n",
    "                        binned_high_vel = binned_high_vel[inlier_indices_3d]\n",
    "                        #high dim\n",
    "                        embeddings_high_dim = embeddings_high_dim[inlier_indices_high_dim, :]\n",
    "                        binned_hipp_angle_high_dim = binned_hipp_angle_high_dim[inlier_indices_high_dim]\n",
    "\n",
    "                    #print(f\"Output embeddings_2d shape after outlier removal: {embeddings_2d.shape}\")\n",
    "                    print(f\"Output embeddings_3d shape after outlier removal: \"\n",
    "                            \"{embeddings_3d.shape}\")\n",
    "\n",
    "                    print(f\"binned_est_gain: {binned_est_gain[100:130]}\")\n",
    "\n",
    "                    # Convert angles to radians\n",
    "                    binned_true_angle_rad = np.deg2rad(binned_true_angle)\n",
    "                    binned_hipp_angle_rad = np.deg2rad(binned_hipp_angle)\n",
    "                    binned_rel_angle_rad = np.deg2rad(binned_rel_angle)\n",
    "                    binned_high_vel_rad = np.deg2rad(binned_high_vel)\n",
    "                    binned_hipp_angle_rad_high_dim = np.deg2rad(binned_hipp_angle_high_dim)\n",
    "\n",
    "                    print(f'binned true angle: {binned_true_angle_rad[:30]}')\n",
    "                    print(f'binned hipp angle: {binned_hipp_angle_rad[:30]}')\n",
    "                    print(f'binned rel angle: {binned_rel_angle_rad[:30]}')\n",
    "                    print(f'binned high vel: {binned_high_vel_rad[:30]}')\n",
    "\n",
    "                    binned_true_angle_rad_unwrap = binned_true_angle_rad\n",
    "                    binned_hipp_angle_rad_unwrap = binned_hipp_angle_rad\n",
    "                    binned_rel_angle_rad_unwrap = binned_rel_angle_rad\n",
    "                    binned_hipp_angle_high_dim_unwrap = binned_hipp_angle_high_dim\n",
    "\n",
    "                    #wrap angles within [0, 2Ï€]\n",
    "                    binned_true_angle_rad = (binned_true_angle_rad_unwrap \n",
    "                                             % (2 * np.pi))\n",
    "                    binned_hipp_angle_rad = (binned_hipp_angle_rad_unwrap \n",
    "                                             % (2 * np.pi))\n",
    "                    binned_rel_angle_rad = binned_rel_angle % (2 * np.pi)\n",
    "\n",
    "                    binned_hipp_angle_rad_high_dim = (binned_hipp_angle_high_dim_unwrap \n",
    "                                                      % (2 * np.pi))\n",
    "\n",
    "                    print(f\"binned_hipp_angle_rad: {binned_hipp_angle_rad[200:230]}\")\n",
    "                    print(f\"binned_true_angle_rad: {binned_true_angle_rad[200:230]}\")\n",
    "\n",
    "                    plot_and_save_behav_vars(\n",
    "                        binned_hipp_angle=binned_hipp_angle_rad, \n",
    "                        binned_true_angle=binned_true_angle_rad, \n",
    "                        binned_est_gain=binned_est_gain,\n",
    "                        save_dir=behav_var_plot_path, \n",
    "                        session_idx=session_idx\n",
    "                    )\n",
    "\n",
    "                    # plot_in_2d(\n",
    "                    #     embeddings=embeddings_2d,\n",
    "                    #     session=session, \n",
    "                    #     behav_var=binned_hipp_angle_rad, \n",
    "                    #     name_behav_var=\"Hipp_Angle\",\n",
    "                    #     principal_curve=None\n",
    "                    # )\n",
    "\n",
    "                    # Compute SI\n",
    "                    SI_params = {\n",
    "                        'n_bins': 10,\n",
    "                        'n_neighbors': 15,\n",
    "                        'discrete_label': False,\n",
    "                        'num_shuffles': 10,\n",
    "                        'verbose': False,\n",
    "                    }\n",
    "\n",
    "                    print(f\"length of embeddings_3d: {embeddings_3d.shape[0]}\")\n",
    "\n",
    "                    SI_score_hipp = compute_SI_and_plot(\n",
    "                        embeddings=embeddings_3d,\n",
    "                        behav_var=binned_hipp_angle_rad,\n",
    "                        params=SI_params,\n",
    "                        behav_var_name='Hipp_Angle',\n",
    "                        save_dir=SI_save_path,\n",
    "                        session_idx=session_idx,\n",
    "                        dimensions_3=True,\n",
    "                        pdf=pdf,\n",
    "                        num_used_clusters=num_used_cluster\n",
    "                    )\n",
    "\n",
    "                    if(SI_score_hipp >= 0.8):\n",
    "                        break\n",
    "                    elif(SI_score_hipp < 0.80):\n",
    "                        print(f\"SI_score_hipp is {SI_score_hipp}.\"\n",
    "                              \" Retrying another embedding\")\n",
    "                        num_reruns += 1\n",
    "                        continue\n",
    "\n",
    "                SI_score_true = compute_SI_and_plot(\n",
    "                    embeddings=embeddings_3d,\n",
    "                    behav_var=binned_true_angle_rad,\n",
    "                    params=SI_params,\n",
    "                    behav_var_name='True_Angle',\n",
    "                    save_dir=SI_save_path,\n",
    "                    session_idx=session_idx,\n",
    "                    dimensions_3=True,\n",
    "                    pdf=pdf,\n",
    "                    num_used_clusters=num_used_cluster\n",
    "                )\n",
    "\n",
    "                penalty_types = ['curvature']\n",
    "                curvature_coeffs = [5]\n",
    "\n",
    "                for penalty_type in penalty_types:\n",
    "                    for curv in curvature_coeffs:\n",
    "                        fit_params = {\n",
    "                            'dalpha': 0.005,\n",
    "                            'knot_order': 'nearest',\n",
    "                            'penalty_type': penalty_type,  # curvature penalty\n",
    "                            'nKnots': 20,\n",
    "                            'curvature_coeff': curv,\n",
    "                            'len_coeff': 2,\n",
    "                            'density_coeff': 2,\n",
    "                            'delta': 0.1\n",
    "                        }\n",
    "\n",
    "                        principal_curve_3d, principal_curve_3d_pre, curve_params_3d = fit_spud_to_cebra(\n",
    "                            embeddings=embeddings_3d,\n",
    "                            ref_angle=binned_true_angle_rad_unwrap,  # Adjust as needed\n",
    "                            session_idx=session_idx,\n",
    "                            session=session,\n",
    "                            results_save_path=results_save_path,\n",
    "                            fit_params=fit_params,\n",
    "                            dimension_3d=1,\n",
    "                            verbose=False\n",
    "                        )\n",
    "\n",
    "                        # Create Rotating 3D Plots\n",
    "                        anim_save_file = f\"{anim_save_path}/bin_size-{bin_size}/\"\n",
    "                        \"vel_thresh_{vel_threshold}/\"\n",
    "                        \"curv_{curv}_pen_type_{penalty_type}/\"\n",
    "                        os.makedirs(anim_save_file, exist_ok=True)\n",
    "\n",
    "\n",
    "                        # Distance and Decoding Calculations\n",
    "                        mean_dist_to_princ = dist_tot_to_princ_curve(\n",
    "                                                                    embeddings=embeddings_3d, \n",
    "                                                                    principal_curve=principal_curve_3d)\n",
    "                        shuffled_hipp_angle_binned_3d = np.random.permutation(binned_hipp_angle_rad)\n",
    "\n",
    "                        decoded_angles, mse_decode_vs_true = calculate_average_difference_in_decoded_hipp_angle(\n",
    "                            embeddings=embeddings_3d,\n",
    "                            principal_curve=principal_curve_3d,\n",
    "                            tt=curve_params_3d,\n",
    "                            behav_angles=binned_hipp_angle_rad,\n",
    "                            true_angles=binned_true_angle_rad\n",
    "                        )\n",
    "                        shuffled_decoded_angles, shuffled_mse_decode_vs_true = calculate_average_difference_in_decoded_hipp_angle(\n",
    "                            embeddings=embeddings_3d,\n",
    "                            principal_curve=principal_curve_3d,\n",
    "                            tt=curve_params_3d,\n",
    "                            behav_angles=shuffled_hipp_angle_binned_3d,\n",
    "                            true_angles=binned_true_angle_rad \n",
    "                        )\n",
    "\n",
    "                \n",
    "                        decoded_angles_unwrap = (\n",
    "                            decoded_angles_unwrap + binned_true_angle_rad_unwrap[3]\n",
    "                        )\n",
    "\n",
    "                        shuffled_decoded_angles_unwrap = (\n",
    "                            shuffled_decoded_angles_unwrap + binned_true_angle_rad_unwrap[3]\n",
    "                        )\n",
    "\n",
    "                        decoded_angles = (\n",
    "                            (decoded_angles + binned_true_angle_rad_unwrap[3]) % (2 * np.pi)\n",
    "                        )\n",
    "\n",
    "                        shuffled_decoded_angles = (\n",
    "                            (shuffled_decoded_angles + binned_true_angle_rad_unwrap[3]) % (2 * np.pi)\n",
    "                        )\n",
    "\n",
    "\n",
    "                        print(\"DECODED_ANGLES\")\n",
    "                        print(decoded_angles_unwrap[:30])\n",
    "\n",
    "                        print('post decodded angles')\n",
    "\n",
    "                        print(decoded_angles_unwrap[:30])\n",
    "\n",
    "\n",
    "                        angle_diff = ((decoded_angles - binned_hipp_angle_rad) \n",
    "                                      % (2 * np.pi))\n",
    "\n",
    "                        shuffled_angle_diff = ((shuffled_decoded_angles - binned_hipp_angle_rad) \n",
    "                                               % (2*np.pi))\n",
    "\n",
    "                        print(angle_diff[:30])\n",
    "                        mean_angle_diff = np.mean(angle_diff)\n",
    "                        shuffled_mean_angle_diff = np.mean(shuffled_angle_diff)\n",
    "\n",
    "                        # # Calculate average angular difference\n",
    "                        # avg_diff = calculate_average_angle_difference(decoded_angles, actual_angles)\n",
    "\n",
    "                        # Apply low-pass filter\n",
    "                        filtered_decoded_angles_unwrap = low_pass_filter(\n",
    "                            angles=decoded_angles_unwrap,\n",
    "                            cutoff_frequency=0.2,\n",
    "                            filter_order=3,\n",
    "                            fs=1\n",
    "                        )\n",
    "\n",
    "                        filtered_decoded_angles_unwrap = savgol_filter(\n",
    "                            filtered_decoded_angles_unwrap,\n",
    "                            window_length=30,\n",
    "                            polyorder=2\n",
    "                        )\n",
    "\n",
    "                        derivative_decoded_angle_rad_unwrap = window_smooth(\n",
    "                            data=filtered_decoded_angles_unwrap,\n",
    "                            window_size=60\n",
    "                        )\n",
    "                        derivative_true_angle_rad_unwrap = window_smooth(\n",
    "                            data=binned_true_angle_rad_unwrap,\n",
    "                            window_size=60\n",
    "                        )\n",
    "                        derivative_hipp_angle_rad_unwrap = window_smooth(\n",
    "                            data=binned_hipp_angle_rad_unwrap,\n",
    "                            window_size=60\n",
    "                        )\n",
    "\n",
    "                \n",
    "                        # Plotting the results\n",
    "                        plt.figure(figsize=(15, 6))\n",
    "                        plt.plot(decoded_angles_unwrap, label='Original Decoded Angles', alpha=0.5)\n",
    "                        plt.plot(shuffled_decoded_angles_unwrap,label='Shuffled Decoded Angles', alpha=0.5)\n",
    "                        plt.plot(filtered_decoded_angles_unwrap, label='Filtered Decoded Angles', linewidth=2)\n",
    "                        plt.xlabel('Time (s)')\n",
    "                        plt.ylabel('Decoded Angle (rad)')\n",
    "                        plt.title('Low-Pass Filter Applied to Decoded Angles')\n",
    "                        plt.legend()\n",
    "                        plt.grid(True)\n",
    "                        pdf.savefig()\n",
    "                        plt.show()\n",
    "\n",
    "                        # Plot Decoded Variables vs true\n",
    "\n",
    "                        plot_decoded_var_and_true(decoded_var=filtered_decoded_angles_unwrap,\n",
    "                            behav_var=binned_hipp_angle_rad_unwrap,\n",
    "                            save_path=param_plot_path,\n",
    "                            session_idx=session_idx,\n",
    "                            behav_var_name='Hippocampal Angle', pdf=pdf)\n",
    "                        \n",
    "                        #interactive plots\n",
    "\n",
    "                        # plot_decoded_var_and_true_interactive(\n",
    "                        #     decoded_var=decoded_angles_unwrap,\n",
    "                        #     behav_var=binned_hipp_angle_rad_unwrap,\n",
    "                        #     true_angle=binned_true_angle_rad_unwrap,\n",
    "                        #     xlabel='Parametrization (0 to 1)',\n",
    "                        #     ylabel1='Decoded Variable',\n",
    "                        #     ylabel2='hipp angle',\n",
    "                        #     legend_labels=['Decoded Variable', 'hipp angle','True Angle'],\n",
    "                        #     save_path=param_plot_path,\n",
    "                        #     session_idx=session_idx,\n",
    "                        #     behav_var_name='Hipp_Angle'\n",
    "                        # )\n",
    "\n",
    "                        # plot_decoded_var_and_true_interactive(\n",
    "                        #     decoded_var=shuffled_decoded_angles_unwrap,\n",
    "                        #     behav_var=binned_hipp_angle_rad_unwrap,\n",
    "                        #     true_angle=binned_true_angle_rad_unwrap,\n",
    "                        #     xlabel='Parametrization (0 to 1)',\n",
    "                        #     ylabel1='Shuffled Decoded Variable',\n",
    "                        #     ylabel2='hipp angle',\n",
    "                        #     legend_labels=['Shuffled Decoded Variable', 'hipp angle','True Angle'],\n",
    "                        #     save_path=param_plot_path,\n",
    "                        #     session_idx=session_idx,\n",
    "                        #     behav_var_name='shuffled_Hipp_Angle'\n",
    "                        # )\n",
    "\n",
    "                        # plot_decoded_var_and_true_interactive(\n",
    "                        #     decoded_var=filtered_decoded_angles_unwrap,\n",
    "                        #     behav_var=binned_hipp_angle_rad_unwrap,\n",
    "                        #     true_angle=binned_true_angle_rad_unwrap,\n",
    "                        #     xlabel='Parametrization (0 to 1)',\n",
    "                        #     ylabel1='Filtered Decoded Variable',\n",
    "                        #     ylabel2='Hipp angle',\n",
    "                        #     legend_labels=['Filtered Decoded Variable', 'Hipp angle','True Angle'],\n",
    "                        #     save_path=param_plot_path,\n",
    "                        #     session_idx=session_idx,\n",
    "                        #     behav_var_name='Hipp_Angle_filtered'\n",
    "                        # )\n",
    "\n",
    "                        # plot_decoded_var_and_true_interactive(\n",
    "                        #     decoded_var=derivative_decoded_angles_unwrap,\n",
    "                        #     behav_var=derivative_hipp_angle_unwrap,\n",
    "                        #     true_angle=derivative_true_angle_unwrap,\n",
    "                        #     xlabel='Parametrization (0 to 1)',\n",
    "                        #     ylabel1='derivatives of filtered angles',\n",
    "                        #     ylabel2='Behavioral Variable',\n",
    "                        #     legend_labels=['Decoded Variable', 'Hipp Angle Derivative','True Angle Derivative'],\n",
    "                        #     save_path=param_plot_path,\n",
    "                        #     session_idx=session_idx,\n",
    "                        #     behav_var_name=\"derivatives\"\n",
    "                        # )\n",
    "\n",
    "                        print(f\"angle_diff random 10: {angle_diff[20:30]}\")\n",
    "                        compare_angles = np.column_stack((\n",
    "                            shuffled_hipp_angle_binned_3d,\n",
    "                            binned_hipp_angle_rad,\n",
    "                            decoded_angles\n",
    "                        ))\n",
    "\n",
    "                        print(f\"compare angle random 10: {compare_angles[20:30]}\")\n",
    "\n",
    "                        plot_in_3d_static(\n",
    "                            embeddings_3d=embeddings_3d,\n",
    "                            session=session,\n",
    "                            behav_var=binned_hipp_angle_rad,\n",
    "                            name_behav_var=\"Hippocampal_Angle\",\n",
    "                            save_path=anim_save_file,\n",
    "                            principal_curve=principal_curve_3d,\n",
    "                            tt=None,\n",
    "                            num_labels=20,\n",
    "                            mean_dist=mean_dist_to_princ,\n",
    "                            avg_angle_diff=mean_angle_diff,\n",
    "                            shuffled_avg_angle_diff=shuffled_mean_angle_diff,\n",
    "                            pdf=pdf\n",
    "                        )\n",
    "\n",
    "                        plot_in_3d_static(\n",
    "                            embeddings_3d=embeddings_3d,\n",
    "                            session=session,\n",
    "                            behav_var=binned_hipp_angle_rad,\n",
    "                            name_behav_var=\"Hippocampal_Angle \",\n",
    "                            save_path=anim_save_file,\n",
    "                            tt=None,\n",
    "                            num_labels=20,\n",
    "                            mean_dist=mean_dist_to_princ,\n",
    "                            avg_angle_diff=mean_angle_diff,\n",
    "                            shuffled_avg_angle_diff=shuffled_mean_angle_diff,\n",
    "                            pdf=pdf\n",
    "                        )\n",
    "\n",
    "                        # Create Rotating 3D Plot\n",
    "                        create_rotating_3d_plot(\n",
    "                            embeddings_3d=embeddings_3d,\n",
    "                            session=session,\n",
    "                            behav_var=binned_hipp_angle_rad,\n",
    "                            name_behav_var=\"hipp angle\",\n",
    "                            anim_save_path=anim_save_file,\n",
    "                            save_anim=save_anim,\n",
    "                            principal_curve=principal_curve_3d,\n",
    "                            tt=curve_params_3d,\n",
    "                            num_labels=20,\n",
    "                            mean_dist=mean_dist_to_princ,\n",
    "                            avg_angle_diff=mean_angle_diff,\n",
    "                            shuffled_avg_angle_diff=shuffled_mean_angle_diff\n",
    "                        )\n",
    "                        print(f\"Created rotating 3D plots for session {session_idx}.\")\n",
    "\n",
    "                # Decoding H\n",
    "                decode_H = (derivative_decoded_angle_rad_unwrap) / (derivative_true_angle_rad_unwrap)\n",
    "\n",
    "                decode_H_nan_count = np.isnan(decode_H).sum()\n",
    "                decode_H_nan_indices = np.where(np.isnan(decode_H))[0]\n",
    "\n",
    "                # For est_gain_binned\n",
    "                est_gain_nan_count = np.isnan(binned_est_gain).sum()\n",
    "                est_gain_nan_indices = np.where(np.isnan(binned_est_gain))[0]\n",
    "\n",
    "                # Print results\n",
    "                print(f\"Mean of decode H (ignoring NaNs): {np.nanmean(decode_H)}\")\n",
    "                print(f\"Mean of est gain H (ignoring NaNs): {np.nanmean(binned_est_gain)}\")\n",
    "\n",
    "                print(f\"Number of NaNs in decode H: {decode_H_nan_count}\")\n",
    "                print(f\"Indices of NaNs in decode H: {decode_H_nan_indices}\")\n",
    "\n",
    "                print(f\"Number of NaNs in est gain H: {est_gain_nan_count}\")\n",
    "                print(f\"Indices of NaNs in est gain H: {est_gain_nan_indices}\")\n",
    "\n",
    "                decode_H = np.clip(decode_H, -2, 2)\n",
    "\n",
    "                min_len = min(len(binned_est_gain), len(decode_H))\n",
    "\n",
    "                mean_H_diff = np.mean(np.abs(binned_est_gain[:min_len] - decode_H[:min_len]))\n",
    "                std_H_diff = np.std(binned_est_gain[:min_len] - decode_H[:min_len])\n",
    "\n",
    "                lap_number, sorted_decode_H, sorted_lap_number = get_var_over_lap(\n",
    "                    var=decode_H,\n",
    "                    true_angle=binned_true_angle_rad_unwrap\n",
    "                )\n",
    "\n",
    "                _, sorted_H_est, _ = get_var_over_lap(\n",
    "                    var=binned_est_gain,\n",
    "                    true_angle=binned_true_angle_rad_unwrap\n",
    "                )\n",
    "\n",
    "                _, sorted_vel, _ = get_var_over_lap(\n",
    "                    var=binned_high_vel,\n",
    "                    true_angle=binned_true_angle_rad_unwrap\n",
    "                )\n",
    "\n",
    "                _, sorted_rel_angle, _ = get_var_over_lap(\n",
    "                    var=binned_rel_angle_rad,\n",
    "                    true_angle=binned_true_angle_rad_unwrap\n",
    "                )\n",
    "\n",
    "                _, sorted_hipp_angle, _ = get_var_over_lap(\n",
    "                    var=binned_hipp_angle_rad,\n",
    "                    true_angle=binned_true_angle_rad_unwrap\n",
    "                )\n",
    "\n",
    "                # get hipp angle laps for spatial spectrogram\n",
    "                hipp_lap_number, hipp_decode_H, sorted_hipp_lap_number = get_var_over_lap(\n",
    "                    var=decode_H,\n",
    "                    true_angle=binned_hipp_angle_rad_unwrap\n",
    "                )\n",
    "                # Plot spatial spectrogram\n",
    "                # True frame\n",
    "                plot_spatial_spectrogram(\n",
    "                    H=sorted_decode_H,\n",
    "                    lap_numbers=sorted_lap_number,\n",
    "                    behav_var_name=\"True_Frame\",\n",
    "                    save_path=spectrogram_path,\n",
    "                    num_segments_per_lap=10,\n",
    "                )\n",
    "\n",
    "                # Hipp frame\n",
    "                plot_spatial_spectrogram(\n",
    "                    H=hipp_decode_H,\n",
    "                    lap_numbers=sorted_hipp_lap_number,\n",
    "                    behav_var_name=\"Hipp_Frame\",\n",
    "                    save_path=spectrogram_path,\n",
    "                    num_segments_per_lap=10,\n",
    "                )\n",
    "\n",
    "                # Interactive plots\n",
    "                plot_spatial_spectrogram_interactive(\n",
    "                    H=sorted_decode_H,\n",
    "                    lap_numbers=sorted_lap_number,\n",
    "                    behav_var_name=\"True_Frame\",\n",
    "                    save_path=spectrogram_path,\n",
    "                    session=session,\n",
    "                    session_idx=session_idx,\n",
    "                    num_segments_per_lap=10,\n",
    "                )\n",
    "\n",
    "                plot_spatial_spectrogram_interactive(\n",
    "                    H=hipp_decode_H,\n",
    "                    lap_numbers=sorted_hipp_lap_number,\n",
    "                    behav_var_name=\"Hipp_Frame\",\n",
    "                    save_path=spectrogram_path,\n",
    "                    session=session,\n",
    "                    session_idx=session_idx,\n",
    "                    num_segments_per_lap=10,\n",
    "                )\n",
    "\n",
    "                print(f\"sorted lap number: {np.max(sorted_lap_number) - np.min(sorted_lap_number)}\")\n",
    "                print(f\"max lap number: {np.max(sorted_lap_number)}\")\n",
    "                print(f\"min lap number: {np.min(sorted_lap_number)}\")\n",
    "\n",
    "                # Plot Hs Over Laps\n",
    "\n",
    "                sorted_H_est_ma = compute_moving_average(data=sorted_H_est, window_size=20)\n",
    "                sorted_decode_H_ma = compute_moving_average(data=sorted_decode_H, window_size=20)\n",
    "\n",
    "            \n",
    "                # Plot Hs Over Laps with Moving Average\n",
    "                \n",
    "                plot_Hs_over_laps_interactive(\n",
    "                    est_H=sorted_H_est_ma,\n",
    "                    decode_H=sorted_decode_H_ma,\n",
    "                    lap_number=sorted_lap_number,\n",
    "                    behav_var=sorted_vel,\n",
    "                    session_idx=session_idx,\n",
    "                    session=session,\n",
    "                    save_path=results_save_path,\n",
    "                    tag='vel_ma_20',\n",
    "                    SI_score=SI_score_hipp,\n",
    "                    decode_err=mse_decode_vs_true,\n",
    "                    mean_diff=mean_H_diff,\n",
    "                    std_diff=std_H_diff,\n",
    "                    behav_var_name=\"Velocity\"\n",
    "                )\n",
    "\n",
    "                plot_Hs_over_laps_interactive(\n",
    "                    est_H=sorted_H_est,\n",
    "                    decode_H=sorted_decode_H,\n",
    "                    lap_number=sorted_lap_number,\n",
    "                    session_idx=session_idx,\n",
    "                    session=session,\n",
    "                    save_path=results_save_path,\n",
    "                    tag='vel_no_ma',\n",
    "                    SI_score=SI_score_hipp,\n",
    "                    decode_err=mse_decode_vs_true,\n",
    "                    mean_diff=mean_H_diff,\n",
    "                    std_diff=std_H_diff,\n",
    "                    behav_var_name=\"Velocity\"\n",
    "                )\n",
    "            \n",
    "                # Plot Hs Over Laps with Moving Average\n",
    "                \n",
    "                plot_Hs_over_laps_interactive(\n",
    "                    est_H=sorted_H_est_ma,\n",
    "                    decode_H=sorted_decode_H_ma,\n",
    "                    lap_number=sorted_lap_number,\n",
    "                    behav_var=sorted_rel_angle,\n",
    "                    session_idx=session_idx,\n",
    "                    session=session,\n",
    "                    save_path=results_save_path,\n",
    "                    tag='rel_ma_20',\n",
    "                    SI_score=SI_score_hipp,\n",
    "                    decode_err=mse_decode_vs_true,\n",
    "                    mean_diff=mean_H_diff,\n",
    "                    std_diff=std_H_diff,\n",
    "                    behav_var_name=\"Rel_Angle\"\n",
    "                )\n",
    "\n",
    "\n",
    "                plot_Hs_over_time(\n",
    "                    est_H=sorted_H_est_ma,\n",
    "                    decode_H=sorted_decode_H_ma,\n",
    "                    session_idx=session_idx,\n",
    "                    behav_var=None,\n",
    "                    behav_var_name='Rel_Angle',\n",
    "                    session=session,\n",
    "                    save_path=results_save_path,\n",
    "                    tag='rel_ma_20',\n",
    "                    is_moving_avg=False,\n",
    "                    SI_score=SI_score_hipp,\n",
    "                    decode_err=mse_decode_vs_true,\n",
    "                    pdf=pdf\n",
    "                )\n",
    "\n",
    "                binned_est_gain_ma = compute_moving_average(data=binned_est_gain, window_size=20)\n",
    "                decode_H_ma = compute_moving_average(decode_H, window_size=20)\n",
    "\n",
    "                #moving avg with vel over time\n",
    "\n",
    "                plot_Hs_over_time(\n",
    "                    est_H=binned_est_gain_ma,\n",
    "                    decode_H=decode_H_ma,\n",
    "                    session_idx=session_idx,\n",
    "                    behav_var=None,\n",
    "                    behav_var_name='Velocity',\n",
    "                    session=session,\n",
    "                    save_path=results_save_path,\n",
    "                    tag='vel_ma_20',\n",
    "                    is_moving_avg=True,\n",
    "                    SI_score=SI_score_hipp,\n",
    "                    decode_err=mse_decode_vs_true\n",
    "                )\n",
    "                \n",
    "                all_neural_data.append(neural_data)\n",
    "                all_embeddings_3d.append(embeddings_3d)\n",
    "                all_principal_curves_3d.append(principal_curve_3d)\n",
    "                all_curve_params_3d.append(curve_params_3d)\n",
    "                all_binned_hipp_angle.append(binned_hipp_angle_rad_unwrap)\n",
    "                all_binned_true_angle.append(binned_true_angle_rad_unwrap)\n",
    "                all_binned_est_gain.append(binned_est_gain)\n",
    "                all_binned_high_vel.append(binned_high_vel)\n",
    "                all_decoded_angles.append(decoded_angles_unwrap)\n",
    "                all_filtered_decoded_angles_unwrap.append(filtered_decoded_angles_unwrap)\n",
    "                all_decode_H.append(decode_H)\n",
    "                all_session_idx.append(session_idx)\n",
    "                all_rat.append(session.rat)\n",
    "                all_day.append(session.day)\n",
    "                all_epoch.append(session.epoch)\n",
    "                all_num_skipped_clusters.append(num_skipped_cluster)\n",
    "                all_num_used_clusters.append(num_used_cluster)\n",
    "                all_avg_skipped_cluster_isolation_quality.append(avg_skipped_cluster_isolation_quality)\n",
    "                all_avg_used_cluster_isolation_quality.append(avg_used_cluster_isolation_quality)\n",
    "                all_mean_distance_to_principal_curve.append(mean_dist_to_princ)\n",
    "                all_mean_angle_difference.append(mean_angle_diff)\n",
    "                all_shuffled_mean_angle_difference.append(shuffled_mean_angle_diff)\n",
    "                all_SI_score_hipp.append(SI_score_hipp)\n",
    "                all_SI_score_true.append(SI_score_true)\n",
    "                all_mse_decode_vs_true.append(mse_decode_vs_true)\n",
    "                all_mean_H_difference.append(mean_H_diff)\n",
    "                all_std_H_difference.append(std_H_diff)\n",
    "\n",
    "                # csv_dict = {\n",
    "                # 'session_idx': session_idx,\n",
    "                # 'rat': session.rat,\n",
    "                # 'day': session.day,\n",
    "                # 'epoch': session.epoch,\n",
    "                # 'num_skipped_clusters': num_skipped_cluster,\n",
    "                # 'num_used_clusters': num_used_cluster,\n",
    "                # 'avg_skipped_cluster_isolation_quality': avg_skipped_cluster_isolation_quality,\n",
    "                # 'avg_used_cluster_isolation_quality': avg_used_cluster_isolation_quality,\n",
    "                # 'mean_distance_to_principal_curve': mean_dist_to_princ,\n",
    "                # 'mean_angle_difference': mean_angle_diff,\n",
    "                # 'shuffled_mean_angle_difference': shuffled_mean_angle_diff,\n",
    "                # 'SI_score_hipp': SI_score_hipp,\n",
    "                # 'SI_score_true': SI_score_true, \n",
    "                # 'SI_score_high_dim': SI_score_high_dim,\n",
    "                # 'mse_decode_vs_true': mse_decode_vs_true,\n",
    "                # 'mean_H_difference': mean_H_diff,\n",
    "                # 'std_H_difference': std_H_diff\n",
    "                # }\n",
    "\n",
    "                # csv_dict_all_data.append(csv_dict)\n",
    "\n",
    "                # # Call the save_csv_data function with the dictionary\n",
    "                # save_data_to_csv(csv_dict, csv_save_path, is_list=False)\n",
    "                # save_data_to_csv(csv_dict_all_data, compile_path, is_list=True) \n",
    "\n",
    "                data_dict = {\n",
    "                    'neural_data': all_neural_data,\n",
    "                    'sessions_metadata': all_sessions_data,\n",
    "                    'embeddings_3d': all_embeddings_3d,\n",
    "                    'principal_curves_3d': all_principal_curves_3d,\n",
    "                    'curve_params_3d': all_curve_params_3d,\n",
    "                    'binned_hipp_angle': all_binned_hipp_angle,\n",
    "                    'binned_true_angle': all_binned_true_angle,\n",
    "                    'binned_est_gain': all_binned_est_gain,\n",
    "                    'binned_high_vel': all_binned_high_vel,\n",
    "                    'decoded_angles': all_decoded_angles,\n",
    "                    'filtered_decoded_angles_unwrap': all_filtered_decoded_angles_unwrap,\n",
    "                    'decode_H': all_decode_H,\n",
    "                    'session_idx': all_session_idx,\n",
    "                    'rat': all_rat,\n",
    "                    'day': all_day,\n",
    "                    'epoch': all_epoch,\n",
    "                    'num_skipped_clusters': all_num_skipped_clusters,\n",
    "                    'num_used_clusters': all_num_used_clusters,\n",
    "                    'avg_skipped_cluster_isolation_quality': all_avg_skipped_cluster_isolation_quality,\n",
    "                    'avg_used_cluster_isolation_quality': all_avg_used_cluster_isolation_quality,\n",
    "                    'mean_distance_to_principal_curve': all_mean_distance_to_principal_curve,\n",
    "                    'mean_angle_difference': all_mean_angle_difference,\n",
    "                    'shuffled_mean_angle_difference': all_shuffled_mean_angle_difference,\n",
    "                    'SI_score_hipp': all_SI_score_hipp,\n",
    "                    'SI_score_true': all_SI_score_true,\n",
    "                    'mse_decode_vs_true': all_mse_decode_vs_true,\n",
    "                    'mean_H_difference': all_mean_H_difference,\n",
    "                    'std_H_difference': all_std_H_difference\n",
    "                }\n",
    "\n",
    "\n",
    "                # Save all data to same .mat file\n",
    "                mat_filename = os.path.join(base_path, f'{save_folder}_all_sessions_data.mat')\n",
    "                savemat(mat_filename, data_dict)\n",
    "\n",
    "                pdf.close()\n",
    "\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_scatter(\n",
    "    data=data,\n",
    "    x_key=\"num_clusters\",\n",
    "    y_key=\"data_quality_score\",\n",
    "    x_label=\"Number of Clusters\",\n",
    "    y_label=\"Data Quality Score\",\n",
    "    title=\"Number of Clusters vs Data Quality Score\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NRSC510",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
